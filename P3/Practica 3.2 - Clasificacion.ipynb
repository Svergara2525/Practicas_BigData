{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "69DXEfKdWWZ2"
   },
   "source": [
    "# Grado en ciencia de datos - Big Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ocZ57Y38WWZ5"
   },
   "source": [
    "\n",
    "# Práctica 3.2 - Clasificación\n",
    "\n",
    "En esta práctica vamos a ver un problema de clasificación con la librería de Spark MLib.\n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pm_Z7hmZWWZ6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/tmp/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "T88b2H6AWWZ8"
   },
   "source": [
    "Otros imports necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6tBAqsBWWWZ9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from pyspark.sql import Row\n",
    "from test_helper import Test\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0mqVfKCDWWZ9"
   },
   "source": [
    "# Construcción y ajuste de los parámetros de una Pipeline\n",
    "\n",
    "Generalmente, una pipeline de ML incluye una serie de fases: **preprocesamiento**, **extracción de características**, **ajuste del modelo** y **validación**.\n",
    "\n",
    "Por ejemplo, para clasificar documentos de texto tendríamos: segmentación/limpieza, extracción de características y el entrenamiento del modelo con validación cruzada para ajustar los parámetros.\n",
    "\n",
    "Aunque existen muchas librerías para cada fase, trabajar con todas no suele ser demasiado fácil, especialmente cuando trabajamos con datasets grandes. Pero, SparkMLib facilita la labor.\n",
    "\n",
    "En esta práctica vamos a ver como afrontar un problema de clasificación de textos simple como ejemplo de una pipeline de ML para clasificación con Spark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b1qjThKmWWZ-"
   },
   "outputs": [],
   "source": [
    "# Imports necesarios de `spark.ml`.\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.param import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.classification import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HT2iI-MeWWZ_"
   },
   "source": [
    "### El dataset \"20 Newsgroups\"\n",
    "\n",
    "En este caso vamos a trabajar con una versión simplificada del dataset 20 newsgroups. Este dataset es una colección de artículos de noticias clasificadas en 20 grupos diferentes.\n",
    "\n",
    "El dataset original se pueden encontrar en  https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups.\n",
    "\n",
    "Para simplificar, el que podéis encontrar en las prácticas es una versión simplificada, donde los 20 grupos los dejamos en 2. Trataremos de saber si un artículo está relacionado con ciencia o no.\n",
    "\n",
    "El dataset que tenéis disponible está ya disponible para leerse como un DataFrame (en el original es necesario leer cada fichero y transformarlo ligeramente). El formato en el que está almacenado es Parquet, el formato binario por defecto utilizado por Spark.\n",
    "\n",
    "Descargar el fichero y dejarlo en datos/20newsgropuBinaryFiltered/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MQvG8iGRWWaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheros en la carpeta: ['.part-r-00000-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet.crc', '.part-r-00001-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet.crc', '.part-r-00002-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet.crc', '.part-r-00003-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet.crc', 'part-r-00000-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet', 'part-r-00001-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet', 'part-r-00002-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet', 'part-r-00003-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet', '_SUCCESS']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dbfs_dir = \"./datos/20newsgropuBinaryFiltered/\"\n",
    "print('Ficheros en la carpeta:', os.listdir(dbfs_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ELrDsRI6WWaD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(len(os.listdir(dbfs_dir)), 9, 'La carpeta contiene 9 ficheros')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "431hklcpWWaE"
   },
   "source": [
    "Utiliza `spark.read.parquet` para leer la carpeta.\n",
    "\n",
    "Posteriormente, utilizando [randomSplit](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) divide el DataFrame en el conjunto de training (60%) y el conjunto de test (40%), utilizando la semilla incluida. (Por porblemas de redondeo y truncado tenemos que usar 0.6035 y .4)\n",
    "\n",
    "Finalmente, cachea los dos DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p7d0wX6HWWaE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, topic: string, id: string, text: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [\"C://Users//alumno//Downloads//Practica3//datos//20newsgropuBinaryFiltered//part-r-00000-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet\",\n",
    "         \"C://Users//alumno//Downloads//Practica3//datos//20newsgropuBinaryFiltered//part-r-00001-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet\",\n",
    "         \"C://Users//alumno//Downloads//Practica3//datos//20newsgropuBinaryFiltered//part-r-00002-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet\",\n",
    "         \"C://Users//alumno//Downloads//Practica3//datos//20newsgropuBinaryFiltered//part-r-00003-ef9ac4e4-a6ba-49ff-817a-eb473f8c07de.snappy.parquet\"]\n",
    "df = spark.read.parquet(*files)\n",
    "# df = spark.read.format(\"parquet\").load(\"file:///C://Users//alumno//Desktop//P3//datos//20newsgropuBinaryFiltered//*\")\n",
    "seed = 12418\n",
    "(training, test) = df.randomSplit([0.6002, 0.4], seed=seed)\n",
    "\n",
    "test.cache()\n",
    "training.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12117"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DCKDANnvWWaE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# Generalmente no hay problemas con este test, pero si no coinciden los números exacto pero se parecen, seguir con la práctica.\n",
    "Test.assertEquals(training.count(), 12117,  \"Número de ejemplos en training incorrecto\")\n",
    "Test.assertEquals(test.count(), 7880, \"Número de ejemplos en test incorrecto\")\n",
    "Test.assertEquals(training.is_cached, True, 'Training no cacheado')\n",
    "Test.assertEquals(test.is_cached, True, 'Test no cacheado')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "px7vum-1WWaE"
   },
   "source": [
    "Vamos a ver qué forma tiene el DataFrame que tenemos disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NajMlIZDWWaF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+--------------------+\n",
      "|label|      topic|   id|                text|\n",
      "+-----+-----------+-----+--------------------+\n",
      "|    0|alt.atheism|49960|From: mathew <mat...|\n",
      "|    0|alt.atheism|51060|From: mathew <mat...|\n",
      "|    0|alt.atheism|51119|From: I3150101@db...|\n",
      "|    0|alt.atheism|51121|From: strom@Watso...|\n",
      "|    0|alt.atheism|51124|From: I3150101@db...|\n",
      "|    0|alt.atheism|51125|From: keith@cco.c...|\n",
      "|    0|alt.atheism|51126|From: keith@cco.c...|\n",
      "|    0|alt.atheism|51127|From: keith@cco.c...|\n",
      "|    0|alt.atheism|51128|From: keith@cco.c...|\n",
      "|    0|alt.atheism|51129|From: keith@cco.c...|\n",
      "+-----+-----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AfoEVDy-WWaF"
   },
   "source": [
    "Podemos explorar el dataset estudiando la distribución de tópicos existente.\n",
    "\n",
    "Para ello crea una consulta que agrupe las noticias por `topic` y cuente cuántas noticias hay en cada uno. Almacena esta información en un DataFrame `topicCount` y muéstrala con show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SzOsdyn1WWaF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               topic|count|\n",
      "+--------------------+-----+\n",
      "|      comp.windows.x|  610|\n",
      "|        misc.forsale|  613|\n",
      "|    rec.sport.hockey|  618|\n",
      "|  rec.sport.baseball|  612|\n",
      "|comp.os.ms-window...|  568|\n",
      "|comp.sys.ibm.pc.h...|  621|\n",
      "|       comp.graphics|  582|\n",
      "|comp.sys.mac.hard...|  587|\n",
      "|     rec.motorcycles|  617|\n",
      "|           rec.autos|  588|\n",
      "|         alt.atheism|  617|\n",
      "|           sci.crypt|  601|\n",
      "|  talk.politics.guns|  613|\n",
      "|  talk.politics.misc|  591|\n",
      "|soc.religion.chri...|  604|\n",
      "|  talk.religion.misc|  635|\n",
      "|talk.politics.mid...|  623|\n",
      "|     sci.electronics|  616|\n",
      "|           sci.space|  599|\n",
      "|             sci.med|  602|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicCount= training.groupBy(\"topic\").count()\n",
    "topicCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic='alt.atheism', count=617),\n",
       " Row(topic='comp.graphics', count=582),\n",
       " Row(topic='comp.os.ms-windows.misc', count=568),\n",
       " Row(topic='comp.sys.ibm.pc.hardware', count=621),\n",
       " Row(topic='comp.sys.mac.hardware', count=587),\n",
       " Row(topic='comp.windows.x', count=610),\n",
       " Row(topic='misc.forsale', count=613),\n",
       " Row(topic='rec.autos', count=588),\n",
       " Row(topic='rec.motorcycles', count=617),\n",
       " Row(topic='rec.sport.baseball', count=612),\n",
       " Row(topic='rec.sport.hockey', count=618),\n",
       " Row(topic='sci.crypt', count=601),\n",
       " Row(topic='sci.electronics', count=616),\n",
       " Row(topic='sci.med', count=602),\n",
       " Row(topic='sci.space', count=599),\n",
       " Row(topic='soc.religion.christian', count=604),\n",
       " Row(topic='talk.politics.guns', count=613),\n",
       " Row(topic='talk.politics.mideast', count=623),\n",
       " Row(topic='talk.politics.misc', count=591),\n",
       " Row(topic='talk.religion.misc', count=635)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(topicCount.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KKLj6gDJWWaF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(sorted(topicCount.collect()), [(u'alt.atheism', 617), (u'comp.graphics', 582), (u'comp.os.ms-windows.misc', 568),\n",
    "                                               (u'comp.sys.ibm.pc.hardware', 621), (u'comp.sys.mac.hardware', 587),\n",
    "                                               (u'comp.windows.x', 610), (u'misc.forsale', 613), (u'rec.autos', 588),\n",
    "                                               (u'rec.motorcycles', 617), (u'rec.sport.baseball', 612),\n",
    "                                               (u'rec.sport.hockey', 618), (u'sci.crypt', 601), (u'sci.electronics', 616),\n",
    "                                               (u'sci.med', 602), (u'sci.space', 599), (u'soc.religion.christian', 604),\n",
    "                                               (u'talk.politics.guns', 613), (u'talk.politics.mideast', 623),\n",
    "                                               (u'talk.politics.misc', 591), (u'talk.religion.misc', 635)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CG4O9opOWWaF"
   },
   "source": [
    "Nuestro objetivo es predecir la etiqueta `label`, es decir, si el artículo está relacionado con la ciencia o no, es interesante observar la distribución de ejemplos por etiqueta.\n",
    "\n",
    "Utiliza para ello el método `groupBy` seguido de `count`para obtener el número de ejemplos por etiqueta que tenemos en el dataset de train. Almacena el resultado en el DataFrame `labelCount` y utiliza show() para mostrarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t4HlHp4uWWaG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2418|\n",
      "|    0| 9699|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelCount = training.groupBy(\"label\").count()\n",
    "labelCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0N4fis0IWWaG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(sorted(labelCount.collect()), [(0, 9699),(1, 2418)], \"Conteo por tópic incorrecto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "51jVbwb_WWaG"
   },
   "source": [
    "### Construcción de la Pipeline para clasificar artículos de noticias\n",
    "\n",
    "Nuestra pipline tendrá las siguientes etapas:\n",
    "\n",
    "1. **RegexTokenizer**, tokeniza cada artículo a secuencias de palabras con un patrón de expresiones regulares,\n",
    "2. **HashingTF**, mapea las secuencias de palabras producidas por RegexTokenizer a vectores de características dispersos usando hashing (no nos dentendremos en ver cómo lo hace, simplemente sabemos que coge listas de palabras y nos da un vectore de características),\n",
    "3. **LogisticRegression**, entrena un modelo de regresión logísticas usando los vectores de características y las etiquetas del conjunto de entrenamiento.\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/ml-Pipeline.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1r97i4dmWWaG"
   },
   "outputs": [],
   "source": [
    "# Debemos construir cada fase de la Pipeline con sus parámetros y finalmente crear la Pipeline\n",
    "\n",
    "# Tokenizer: columna de entrada = test, columna de salida = words y patrón a buscar s+ (dividimos por espacios)\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"s+\")\n",
    "\n",
    "# HashingTF: indicamos la columna de entrada como la de salida del tokenizer, la de salida = features y el número de caraccterísticas a obtener (5000)\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\", numFeatures=5000)\n",
    "\n",
    "# Regresión logística con 20 iteraciones y parámetro de regularización = 0.01\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.01)\n",
    "\n",
    "# Creamos la pipline de ML como una lista de fases\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ev_Lx0JrWWaG"
   },
   "outputs": [],
   "source": [
    "# Una vez construida la pipeline, podemos entrenar el modelo con el conjunto de training\n",
    "# Utiliza fit para ajustar el modelo al training\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H7simJvAWWaG"
   },
   "source": [
    "### Comprobar y evaluar las predicciones\n",
    "\n",
    "Una vez obtenido el PipelineModel, queremos saber cómo se comporta.\n",
    "Primero lo haremos visualizando las etiquetas predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mpuRDBvkWWaH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|                text|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|From: mathew <mat...|\n",
      "|       0.0|    0|From: mathew <mat...|\n",
      "|       0.0|    0|From: I3150101@db...|\n",
      "|       0.0|    0|From: strom@Watso...|\n",
      "|       0.0|    0|From: I3150101@db...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: bobbe@vice....|\n",
      "|       0.0|    0|From: bobbe@vice....|\n",
      "|       0.0|    0|From: joslin@pogo...|\n",
      "|       0.0|    0|From: halat@pooh....|\n",
      "|       0.0|    0|From: halat@pooh....|\n",
      "|       0.0|    0|From: dgraham@bme...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: keith@cco.c...|\n",
      "|       0.0|    0|From: rm03@ic.ac....|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utiliza transform para predecir los resultados sobre el conjunto de training\n",
    "prediction = model.transform(training)\n",
    "\n",
    "# Muestra las etiquetas predichas junto con las reales y el testo (prediction, label y text)\n",
    "# Show the predicted labels along with true labels and raw texts.\n",
    "prediction.select(\"prediction\", \"label\", \"text\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2zbM2Nr3WWaH"
   },
   "source": [
    "Parece que los resultados sobre training son buenos. Pero vamos a ver el resultado cuantitativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ARGHKJweWWaH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997206016907085"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un evaluador para clasificación binaria usando el área bajo la curva ROC\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "\n",
    "# Utiliza el evaluador creado con el método evaluate para obtener el AUC del modelo sobre train\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pjRkai-eWWaH"
   },
   "source": [
    "El resultado en training es prácticamente perfecto (approximadamente 0.99), pero esto suele ser muchas veces una pista de que estamos sobrentrenando. Veamos el resultado sobre test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PLxrTKpWWWaH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090765474882357"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliza el método evaluate del evaluador y el método transform del modelo para obtener las predicciones sobre test y posteriomente evaluarlas\n",
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tSp5fByuWWaI"
   },
   "source": [
    "El AUC sobre test es mucho más pequeño (aproximadamente 0.9).\n",
    "Parece que tenemos algún problema más aparte del sobreentrenamiento. \n",
    "Estudiemos las fases establecidas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rKEDHTnXWWaI"
   },
   "source": [
    "## Comprobación de la Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "71cypCKaWWaI"
   },
   "source": [
    "Las predicciones de la pipline tienen también resultados intermedios de cada fase:\n",
    "* \"words\" del tokenizer,\n",
    "* \"features\" del hashing ,\n",
    "* \"prediction\", \"probability\", y \"rawPredictions\" de la regresión logística.\n",
    "\n",
    "Veamos el esquema de \"prediction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1mj0EcDmWWaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xeZiMIhLWWaJ"
   },
   "source": [
    "Podemos mirar todas las columnas usando show() sobre prediction.\n",
    "\n",
    "¿Qué no nos cuadra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xjG1AdIVWWaJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|      topic|   id|                text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|alt.atheism|49960|From: mathew <mat...|[from: mathew <ma...|(5000,[10,20,40,4...|[18.5233779949098...|[0.99999999097599...|       0.0|\n",
      "|    0|alt.atheism|51060|From: mathew <mat...|[from: mathew <ma...|(5000,[4,5,7,8,11...|[17.2091700147994...|[0.99999996641445...|       0.0|\n",
      "|    0|alt.atheism|51119|From: I3150101@db...|[from: i3150101@d...|(5000,[12,40,45,8...|[8.87573281181239...|[0.99986028033577...|       0.0|\n",
      "|    0|alt.atheism|51121|From: strom@Watso...|[from: , trom@wat...|(5000,[63,331,750...|[2.96662743663277...|[0.95104348987461...|       0.0|\n",
      "|    0|alt.atheism|51124|From: I3150101@db...|[from: i3150101@d...|(5000,[8,12,79,19...|[10.2104858555989...|[0.99996321876044...|       0.0|\n",
      "|    0|alt.atheism|51125|From: keith@cco.c...|[from: keith@cco....|(5000,[4,7,19,27,...|[7.95392521342644...|[0.99964884275113...|       0.0|\n",
      "|    0|alt.atheism|51126|From: keith@cco.c...|[from: keith@cco....|(5000,[295,628,66...|[5.27640027731609...|[0.99491519224605...|       0.0|\n",
      "|    0|alt.atheism|51127|From: keith@cco.c...|[from: keith@cco....|(5000,[101,207,29...|[6.61154458126418...|[0.99865705246987...|       0.0|\n",
      "|    0|alt.atheism|51128|From: keith@cco.c...|[from: keith@cco....|(5000,[404,522,66...|[6.37885155977504...|[0.99830580434770...|       0.0|\n",
      "|    0|alt.atheism|51129|From: keith@cco.c...|[from: keith@cco....|(5000,[120,199,27...|[7.29358372594014...|[0.99932057478009...|       0.0|\n",
      "+-----+-----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcyjAF9dWWaJ"
   },
   "source": [
    "Si nos fijamos bien, la columna \"words\" debería tener un array de strings con palabras, pero sin embargo vemos que algo no ha ido bien. Por lo que parece que el tokenizer no está funcionando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W7aHitHdWWaJ"
   },
   "source": [
    "Vamos a usar `explainParams` sobre el tokenizer para ver los parámetros establecidos y su documentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QrF-kkY8WWaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaps: whether regex splits on gaps (True) or matches tokens (False) (default: True)\n",
      "inputCol: input column name. (current: text)\n",
      "minTokenLength: minimum token length (>= 0) (default: 1)\n",
      "outputCol: output column name. (default: RegexTokenizer_cebed504ffc2__output, current: words)\n",
      "pattern: regex pattern (Java dialect) used for tokenizing (default: \\s+, current: s+)\n",
      "toLowercase: whether to convert all characters to lowercase before tokenizing (default: True)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.explainParams())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tH8q2ZeHWWaK"
   },
   "source": [
    "¡Cuidado! Nos hemos olvidado de la contrabarra en la expresión regular... Debe ser \"\\s+\" y no \"s+\". Vamos a corregirlo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-brxXaDJWWaK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegexTokenizer_cebed504ffc2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliza el método setPattern de tokenizer para poner el patrónSet the value of \"pattern\" back to \"\\s+\",\n",
    "# necesitarás incluir una doble contrabarra \"\\\\s+\"\n",
    "tokenizer.setPattern(\"\\\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kOmvQsbyWWaK"
   },
   "outputs": [],
   "source": [
    "# Entrenamos el modelo de nuevo\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rXwhKQcFWWaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|      topic|   id|                text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|alt.atheism|49960|From: mathew <mat...|[from:, mathew, <...|(5000,[4,7,13,20,...|[12.2856481904012...|[0.99999538248006...|       0.0|\n",
      "|    0|alt.atheism|51060|From: mathew <mat...|[from:, mathew, <...|(5000,[1,4,7,11,1...|[43.7224015257718...|           [1.0,0.0]|       0.0|\n",
      "|    0|alt.atheism|51119|From: I3150101@db...|[from:, i3150101@...|(5000,[2,4,20,54,...|[3.74992972803529...|[0.97702105247291...|       0.0|\n",
      "|    0|alt.atheism|51121|From: strom@Watso...|[from:, strom@wat...|(5000,[41,42,54,8...|[2.69102498368475...|[0.93649496692156...|       0.0|\n",
      "|    0|alt.atheism|51124|From: I3150101@db...|[from:, i3150101@...|(5000,[20,54,76,9...|[4.82715901108167...|[0.99205439513590...|       0.0|\n",
      "|    0|alt.atheism|51125|From: keith@cco.c...|[from:, keith@cco...|(5000,[23,34,54,5...|[10.5674954054152...|[0.99997426146169...|       0.0|\n",
      "|    0|alt.atheism|51126|From: keith@cco.c...|[from:, keith@cco...|(5000,[54,95,120,...|[3.81338672992513...|[0.97840341194678...|       0.0|\n",
      "|    0|alt.atheism|51127|From: keith@cco.c...|[from:, keith@cco...|(5000,[23,54,95,1...|[3.63955059487339...|[0.97440800724216...|       0.0|\n",
      "|    0|alt.atheism|51128|From: keith@cco.c...|[from:, keith@cco...|(5000,[11,52,54,9...|[4.59357400374642...|[0.98998468452287...|       0.0|\n",
      "|    0|alt.atheism|51129|From: keith@cco.c...|[from:, keith@cco...|(5000,[4,52,54,66...|[3.07234195592745...|[0.95573735107547...|       0.0|\n",
      "+-----+-----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprobamso las predicciones y que las palabras tiene buena pinta\n",
    "prediction = model.transform(training)\n",
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Jg-udLXXWWaK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998468372793629"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluamos el modelo en train y en test\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8yhBi3MxWWaL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9579124109393602"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qiqhnfMnWWaL"
   },
   "source": [
    "Ahora sí que parece que funciona mejor, aunque seguimos teniendo algo de sobrentrenamiento (0.999 train y 0.96 test).\n",
    "Para mejorar el error de generalización podemos ajustar los parámetros de la Pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOpeYBxWWaL"
   },
   "source": [
    "## Ajuste de parámetros mediante validación cruzada\n",
    "\n",
    "Podemos usar la validación cruzada en MLlib mediante `CrossValidator`.\n",
    "\n",
    "Se toma una lista de combinaciones de parámetros y una medida de evaluación y él automáticamente busca la mejor combinación mediante validación cruzada.\n",
    "\n",
    "Consulta la siguiente documentación para preparar la validación cruzada.\n",
    "\n",
    "[CrossValidator](http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n",
    "\n",
    "[ParamGridBuilder](http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jYh5tFLDWWaM"
   },
   "outputs": [],
   "source": [
    "# Las combinaciones de parámetros se generan como todas las posibles combinaciones entre los diferentes parámetros establecidos\n",
    "# Para simplificarlo, solo usaremos diferentes parámetros de hashing TF y del parámetro de regularización de la regresión\n",
    "# Utiliza ParamGridBuilder para considerar los siguientes parámetros\n",
    "# hashingTF.numFeatures = [1000, 10000]\n",
    "# lr.regParam = [0.05, 0.2]\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(hashingTF.numFeatures, [1000, 10000]) \\\n",
    "  .addGrid(lr.regParam, [0.05, 0.2]) \\\n",
    "  .build()\n",
    "\n",
    "# Creamos un CrossValidator para ajustar la pipeline\n",
    "# Utilizamos como estimator la pipeline, como evaluator el evaluator ya definido y como parámetros paramGrid.\n",
    "# Para que tarde menos utiliza solo 2 particiones\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1hhTq3AjWWaM"
   },
   "source": [
    "Ajustar un modelo de validación cruzada funciona igual que hacerlo con la Pipeline. Llevará más tiempo porque se ajustan muchos más modelos para elegir el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0MbnvCRjWWaM"
   },
   "outputs": [],
   "source": [
    "# Utiliza fit sobre cv para obtener el modelo entrenado con el conjunto de training\n",
    "cvModel = cv.fit(training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8cWNwKP3WWaM"
   },
   "source": [
    "Veamos los resultados que obtenemos ahora con el evaluador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IzrR87YuWWaN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991397388950843"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero sobre el conjunto training\n",
    "evaluator.evaluate(cvModel.transform(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tSjctEoAWWaN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774924534066357"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora sobre el conjunto test\n",
    "evaluator.evaluate(cvModel.transform(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HMSrg3mPWWaN"
   },
   "source": [
    "¡Hemos mejorado en test! (0.9756 aprox.)\n",
    "Solo hemos probado unas pocas combinaciones de parámetros. Podríamos realizar muchos más experimentos para mejorar todavía más los resultados."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "name": "MLPipeline Newsgroup Dataset",
  "notebookId": 3638908530782612
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
