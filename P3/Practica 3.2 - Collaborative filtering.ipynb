{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grado en ciencia de datos - Big Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3.2- Sistemas de recomendación con collaborative filtering\n",
    "\n",
    "En esta práctica afrontaremos vamos a crear un sistema de recomendación de películas con la librería de Spark MLib.\n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de la puntuación a películas\n",
    "\n",
    "En esta práctica vamos a intentar predecir lo que quieren los usuarios de manera similar a lo que hacen Amazon o Netflix para recomendar los productos o las películas en las que puedes estar interesado. En este caso, vamos a usar Spark para montar un sistema de recomendación que permita recomendar películas a un usuario. Para ello utilizaremos el modelo de filtro colaborativo con el algoritmo Alternating Least Squares (ALS) disponible en Spark MLlib ([sparkml]).\n",
    "\n",
    "Como conjunto de datos utilizaremos [MovieLens](http://grouplens.org/datasets/movielens/) que contiene 20 millones de puntuaciones de usuarios a películas. Descomprime el fichero ml-20m.zip en tu carpeta de datos. Dentro de esta carpeta debe estar la carpeta ml-20m con todos los csv's incluidos en el fichero zip.\n",
    "\n",
    "La práctica está dividida en cuatro partes:\n",
    "* *Parte 0*: Preliminares\n",
    "* *Parte 1*: Recomendaciones básicas\n",
    "* *Parte 2*: Filtro colaborativo\n",
    "* *Parte 3*: Predicciones para ti\n",
    "\n",
    "Como ya hemos comentado en las prácticas anteriores, cuidado antes de usar `collect()` ya que el dataset con el que trabajamos es bastante grande.\n",
    "\n",
    "[sparkml]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 15:46:12 WARN Utils: Your hostname, MacBook-Pro-de-Sergio.local resolves to a loopback address: 127.0.0.1; using 172.19.109.126 instead (on interface en0)\n",
      "25/12/19 15:46:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/19 15:46:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/tmp/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros imports necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from pyspark.sql import Row\n",
    "from test_helper import Test\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas\n",
    "\n",
    "En la práctica solo es necesario usar instrucciones básicas de python y las transformaciones y acciones de DataFrames.\n",
    "\n",
    "**Ejecutar la siguiente celda:** Establecemos los nombres de los ficheros a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from test_helper import Test\n",
    "\n",
    "dbfs_dir = './datos/ml-20m'\n",
    "movies_filename = dbfs_dir + '/movies.csv'\n",
    "ratings_filename = dbfs_dir + '/ratings.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preliminarres\n",
    "\n",
    "Leemos los dos ficheros como DataFrames.\n",
    "\n",
    "El conjunto de datos consiste en varios ficheros CSV con sus cabeceras, por lo que podemos parsearslos fácilmente con Spark.\n",
    "\n",
    "De todos los ficheros disponibles, solo nos interesan dos: ratings.csv y movies.csv. El primero almacena en cada fila la puntuación de un usuario a una película, mientras que el segundo almacena en cada fila la información de una película. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheros en la carpeta: ['links.csv', 'tags.csv', 'genome-tags.csv', 'ratings.csv', 'README.txt', 'genome-scores.csv', 'movies.csv']\n"
     ]
    }
   ],
   "source": [
    "print('Ficheros en la carpeta:', os.listdir(dbfs_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que conocemos el esquema de los ficheros, lo vamos a especificar explícitamente para acelerar la lectura y que Spark no tenga que leer dos veces cada fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType([\n",
    "  StructField(\"userId\", IntegerType(), True),\n",
    "  StructField(\"movieId\", IntegerType(), True),\n",
    "  StructField(\"rating\", FloatType(), True)\n",
    "])\n",
    "    \n",
    "movies_df_schema = StructType([\n",
    "  StructField(\"ID\", IntegerType(), True),\n",
    "  StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos y los cacheamos\n",
    "\n",
    "**Paciencia**: La lectura puede tardar un poco..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 15:46:24 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 3\n",
      "CSV file: file:///Users/vergara/Documents/Master/Computación%20Avanzada/Practicas_BigData/P3/datos/ml-20m/ratings.csv\n",
      "25/12/19 15:46:33 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: file:///Users/vergara/Documents/Master/Computación%20Avanzada/Practicas_BigData/P3/datos/ml-20m/movies.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 20000263 puntuaciones y 27278 películas en el conjunto de datos\n",
      "Puntuaciones:\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      2|   3.5|\n",
      "|     1|     29|   3.5|\n",
      "|     1|     32|   3.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Películas:\n",
      "+---+-----------------------+\n",
      "|ID |title                  |\n",
      "+---+-----------------------+\n",
      "|1  |Toy Story (1995)       |\n",
      "|2  |Jumanji (1995)         |\n",
      "|3  |Grumpier Old Men (1995)|\n",
      "+---+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Leemos el csv de ratings, con header\n",
    "raw_ratings_df = spark.read.csv(ratings_filename, header=True, schema=ratings_df_schema)\n",
    "\n",
    "# Leemos el csv de movies, con header\n",
    "raw_movies_df = spark.read.csv(movies_filename, header=True, schema=movies_df_schema)\n",
    "\n",
    "# Eliminamos las columnas que no interesan\n",
    "ratings_df = raw_ratings_df   # Nos interesan todas.\n",
    "movies_df = raw_movies_df.select('ID', 'title')\n",
    "\n",
    "# Cacheamos los DataFrames ¡¡¡CUIADADO CON EL LABORATORIO!!!!\n",
    "ratings_df.cache()\n",
    "movies_df.cache()\n",
    "\n",
    "# Comprobamos si se han cacheado correctamente.\n",
    "assert ratings_df.is_cached\n",
    "assert movies_df.is_cached\n",
    "\n",
    "# Contamos el número de elementos en cada DataFrame para forzar la lectura y el cacheo\n",
    "raw_ratings_count = raw_ratings_df.count()\n",
    "ratings_count = ratings_df.count()\n",
    "raw_movies_count = raw_movies_df.count()\n",
    "movies_count = movies_df.count()\n",
    "\n",
    "print('Hay %s puntuaciones y %s películas en el conjunto de datos' % (ratings_count, movies_count))\n",
    "print('Puntuaciones:')\n",
    "ratings_df.show(3)\n",
    "\n",
    "print('Películas:')\n",
    "movies_df.show(3, truncate=False)\n",
    "\n",
    "assert raw_ratings_count == ratings_count\n",
    "assert raw_movies_count == movies_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que todo esté correcto, no debería dar ningún error la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ratings_count == 20000263\n",
    "assert movies_count == 27278\n",
    "assert movies_df.filter(movies_df.title == 'Toy Story (1995)').count() == 1\n",
    "assert ratings_df.filter((ratings_df.userId == 6) & (ratings_df.movieId == 1) & (ratings_df.rating == 5.0)).count() == 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a revisar como son los DataFrames que acabamos de leer.\n",
    "\n",
    "**Ejecutar las siguientes celdas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+\n",
      "|ID |title                             |\n",
      "+---+----------------------------------+\n",
      "|1  |Toy Story (1995)                  |\n",
      "|2  |Jumanji (1995)                    |\n",
      "|3  |Grumpier Old Men (1995)           |\n",
      "|4  |Waiting to Exhale (1995)          |\n",
      "|5  |Father of the Bride Part II (1995)|\n",
      "+---+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      2|   3.5|\n",
      "|     1|     29|   3.5|\n",
      "|     1|     32|   3.5|\n",
      "|     1|     47|   3.5|\n",
      "|     1|     50|   3.5|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Recomendaciones básicas\n",
    "\n",
    "Antes de utilizar el filtro colaborativo vamos a realizar unas recomendaciones más simples, basadas en las estadísticas globales. Veremos que la ventaja del filtro colaborativo es que en vez de realizar la misma recomendación a todos los usuarios como haríamos aquí, realizamos recomendaciones personalizadas.\n",
    "\n",
    "Vamos a por la forma simple de recomendación: La forma más simple consiste en recomendar la película con la mejor puntuación media.\n",
    "\n",
    "Usaremos Spark para encontrar el nombre, el número de puntuaciones y la media de las puntuaciones de las 20 películas con la media más alta de puntuación y que tengan al menos 500 puntuaciones. Eliminamos las puntuaciones con menos de 500 puntuaciones ya que es posible que estas no sean del gusto de todo el mundo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Películas con la media de puntuación más alta\n",
    "\n",
    "Estos son los pasos que debes seguir:\n",
    "\n",
    "1. `ratings_df` contiene tres columnas:\n",
    "    - userID: El ID del usuario que ha dado la puntuación\n",
    "    - movieID: El ID de la película puntuada\n",
    "    - rating: la puntuación.\n",
    "\n",
    "   Primero, transforma `ratings_df` en un segundo DataFrame, `movie_ids_with_avg_ratings` con las siguientes columnas:\n",
    "    - El ID de la película\n",
    "    - El número de puntuaciones recibidas por la película\n",
    "    - La media de las puntuaciones recibidas por la película\n",
    "   Para ello considera el uso de GroupBy, junto con agg y las funciones count y avg.\n",
    "\n",
    "2. Transforma `movie_ids_with_avg_ratings` a otro DataFrame, `movie_names_with_avg_ratings_df` que añade el título de la película acada fila. `movie_names_with_avg_ratings_df` contendrá las siguientes columnas:\n",
    "    - El ID de la película\n",
    "    - El título de la película\n",
    "    - El número de puntuaciones recibidas por la película\n",
    "    - La media de las puntuaciones recibidas por la película\n",
    "\n",
    "   **Nota**: Considera el uso de join para unir el DataFrame `movie_ids_with_avg_ratings` y el de las películas `movies_df`\n",
    "\n",
    "El resultado debería ser similar al siguiente:\n",
    "```\n",
    "movie_ids_with_avg_ratings_df:\n",
    "+-------+-----+------------------+\n",
    "|movieId|count|average           |\n",
    "+-------+-----+------------------+\n",
    "|1831   |7463 |2.5785207021305103|\n",
    "|431    |8946 |3.695059244355019 |\n",
    "|631    |2193 |2.7273141814865483|\n",
    "+-------+-----+------------------+\n",
    "only showing top 3 rows\n",
    "\n",
    "movie_names_with_avg_ratings_df:\n",
    "+-------+-----------------------------+-----+-------+\n",
    "|average|title                        |count|movieId|\n",
    "+-------+-----------------------------+-----+-------+\n",
    "|5.0    |Ella Lola, a la Trilby (1898)|1    |94431  |\n",
    "|5.0    |Serving Life (2011)          |1    |129034 |\n",
    "|5.0    |Diplomatic Immunity (2009? ) |1    |107434 |\n",
    "+-------+-----------------------------+-----+-------+\n",
    "only showing top 3 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ids_with_avg_ratings_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+-------+\n",
      "|movieId|count|average           |MovieID|\n",
      "+-------+-----+------------------+-------+\n",
      "|3997   |2047 |2.0703468490473864|3997   |\n",
      "|1580   |35580|3.55831928049466  |1580   |\n",
      "|3918   |1246 |2.918940609951846 |3918   |\n",
      "+-------+-----+------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "movie_names_with_avg_ratings_df:\n",
      "+-------+-----+------------------+-------+--------------------------------+\n",
      "|movieId|count|average           |MovieID|title                           |\n",
      "+-------+-----+------------------+-------+--------------------------------+\n",
      "|3997   |2047 |2.0703468490473864|3997   |Dungeons & Dragons (2000)       |\n",
      "|1580   |35580|3.55831928049466  |1580   |Men in Black (a.k.a. MIB) (1997)|\n",
      "|3918   |1246 |2.918940609951846 |3918   |Hellbound: Hellraiser II (1988) |\n",
      "+-------+-----+------------------+-------+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "# A partir de ratingsDF, crear movie_ids_with_avg_ratings_df donde tengamos \n",
    "# para cada película el conteo de puntuaciones y la media de las mismas\n",
    "movie_ids_with_avg_ratings_df = (\n",
    "    ratings_df.groupBy('movieId')\n",
    "              .agg(functions.count('*').alias('count'),\n",
    "                   functions.avg('rating').alias('average'))\n",
    "              .withColumn('MovieID', functions.col('movieId'))\n",
    "              .select('movieId', 'count', 'average', 'MovieID')\n",
    ")\n",
    "print('movie_ids_with_avg_ratings_df:')\n",
    "movie_ids_with_avg_ratings_df.show(3, truncate=False)\n",
    "\n",
    "# Nota: movie_names_df es una variable temporal, la usaremos para guardar la unión del DataFrame que\n",
    "# acabamos de obtener y el DataFrame movies_df (la unión se debe realizar por los campos movieId e ID)\n",
    "movie_names_df = movie_ids_with_avg_ratings_df.join(\n",
    "    movies_df,\n",
    "    movie_ids_with_avg_ratings_df.movieId == movies_df.ID,\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "# En este segundo paso, eliminar el atributo ID de movie_names usando drop\n",
    "movie_names_with_avg_ratings_df = movie_names_df.drop('ID')\n",
    "print('movie_names_with_avg_ratings_df:')\n",
    "movie_names_with_avg_ratings_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(movie_ids_with_avg_ratings_df.count(), 26744,\n",
    "                'movie_ids_with_avg_ratings_df.count() incorrecto (se esperaban 26744)')\n",
    "movie_ids_with_ratings_take_ordered = movie_ids_with_avg_ratings_df.orderBy('MovieID').take(3)\n",
    "_take_0 = movie_ids_with_ratings_take_ordered[0]\n",
    "_take_1 = movie_ids_with_ratings_take_ordered[1]\n",
    "_take_2 = movie_ids_with_ratings_take_ordered[2]\n",
    "Test.assertTrue(_take_0[0] == 1 and _take_0[1] == 49695,\n",
    "                'Conteo de puntuaciones incorrecto para la película {0} (se esperaban 49695)'.format(_take_0[0]))\n",
    "Test.assertEquals(__builtin__.round(_take_0[2], 2), 3.92, \"Puntuación media incorecta para la película {0}. Se esperaba 3.92\".format(_take_0[0]))\n",
    "\n",
    "Test.assertTrue(_take_1[0] == 2 and _take_1[1] == 22243,\n",
    "                'Conteo de puntuaciones incorrecto para la película {0} (se esperaban 22243)'.format(_take_1[0]))\n",
    "Test.assertEquals(__builtin__.round(_take_1[2], 2), 3.21, \"Puntuación media incorecta para la película {0}. Se esperaba 3.21\".format(_take_1[0]))\n",
    "\n",
    "Test.assertTrue(_take_2[0] == 3 and _take_2[1] == 12735,\n",
    "                'Conteo de puntuaciones incorrecto para la película  {0} (expected 12735)'.format(_take_2[0]))\n",
    "Test.assertEquals(__builtin__.round(_take_2[2], 2), 3.15, \"Puntuación media incorecta para la película {0}. Se esperaba 3.15\".format(_take_2[0]))\n",
    "\n",
    "\n",
    "Test.assertEquals(movie_names_with_avg_ratings_df.count(), 26744,\n",
    "                  'movie_names_with_avg_ratings_df.count() incorrecto (se esperaban 26744)')\n",
    "movie_names_with_ratings_take_ordered = movie_names_with_avg_ratings_df.orderBy(['average', 'title']).take(3)\n",
    "result = [(r['average'], r['title'], r['count'], r['movieId']) for r in movie_names_with_ratings_take_ordered]\n",
    "Test.assertEquals(result,\n",
    "                  [(0.5, u'13 Fighting Men (1960)', 1, 109355),\n",
    "                   (0.5, u'20 Years After (2008)', 1, 131062),\n",
    "                   (0.5, u'3 Holiday Tails (Golden Christmas 2: The Second Tail, A) (2011)', 1, 111040)],\n",
    "                  'Las tres primeras Rows de movie_names_with_avg_ratings_df no son correctas')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Películas con la puntuación más alta y al menos 500 puntuaciones recibidas\n",
    "\n",
    "Ahora que tenemos el DataFrame con las puntuaciones medias podemos usar Spark para obtener las 20 películas con la puntuación media más alta que tengan por lo menos 500 puntuaciones asignadas.\n",
    "\n",
    "Utilizar una única transformación para limiar el resultado de las películas a aquellas que han sido puntuadas como mínimo 500 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Películas con las puntuaciones más altas:\n",
      "+-------+-----+------------------+-------+--------------------------------+\n",
      "|movieId|count|average           |MovieID|title                           |\n",
      "+-------+-----+------------------+-------+--------------------------------+\n",
      "|3997   |2047 |2.0703468490473864|3997   |Dungeons & Dragons (2000)       |\n",
      "|1580   |35580|3.55831928049466  |1580   |Men in Black (a.k.a. MIB) (1997)|\n",
      "|3918   |1246 |2.918940609951846 |3918   |Hellbound: Hellraiser II (1988) |\n",
      "|2366   |6627 |3.5492681454655197|2366   |King Kong (1933)                |\n",
      "|3175   |13945|3.600717102904267 |3175   |Galaxy Quest (1999)             |\n",
      "|4519   |1936 |3.2463842975206614|4519   |Land Before Time, The (1988)    |\n",
      "|1591   |5255 |2.6201712654614653|1591   |Spawn (1997)                    |\n",
      "|471    |11268|3.6641817536386228|471    |Hudsucker Proxy, The (1994)     |\n",
      "|36525  |1169 |3.482891360136869 |36525  |Just Like Heaven (2005)         |\n",
      "|44022  |2465 |3.334077079107505 |44022  |Ice Age 2: The Meltdown (2006)  |\n",
      "|2866   |1407 |3.605188343994314 |2866   |Buddy Holly Story, The (1978)   |\n",
      "|1645   |11458|3.4787484726828417|1645   |Devil's Advocate, The (1997)    |\n",
      "|5803   |1046 |2.772944550669216 |5803   |I Spy (2002)                    |\n",
      "|54190  |1687 |3.6701244813278007|54190  |Across the Universe (2007)      |\n",
      "|1088   |11013|3.209207300463089 |1088   |Dirty Dancing (1987)            |\n",
      "|833    |1427 |2.725998598458304 |833    |High School High (1996)         |\n",
      "|8638   |3449 |3.9375181211945494|8638   |Before Sunset (2004)            |\n",
      "|1959   |5016 |3.628987240829346 |1959   |Out of Africa (1985)            |\n",
      "|1342   |3289 |2.949072666463971 |1342   |Candyman (1992)                 |\n",
      "|1238   |3194 |3.96665623043206  |1238   |Local Hero (1983)               |\n",
      "+-------+-----+------------------+-------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter(movie_names_with_avg_ratings_df['count'] >= 500)                                                           \n",
    "print('Películas con las puntuaciones más altas:')\n",
    "movies_with_500_ratings_or_more.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(movies_with_500_ratings_or_more.count(), 4489,\n",
    "                  'movies_with_500_ratings_or_more.count() incorrecto. Se esperaban 4489.')\n",
    "top_20_results = [(r['average'], r['title'], r['count']) for r in movies_with_500_ratings_or_more.orderBy(desc('average')).take(20)]\n",
    "\n",
    "Test.assertEquals(top_20_results,\n",
    "                  [(4.446990499637029, u'Shawshank Redemption, The (1994)', 63366),\n",
    "                   (4.364732196832306, u'Godfather, The (1972)', 41355),\n",
    "                   (4.334372207803259, u'Usual Suspects, The (1995)', 47006),\n",
    "                   (4.310175010988133, u\"Schindler's List (1993)\", 50054),\n",
    "                   (4.275640557704942, u'Godfather: Part II, The (1974)', 27398),\n",
    "                   (4.2741796572216, u'Seven Samurai (Shichinin no samurai) (1954)', 11611),\n",
    "                   (4.271333600779414, u'Rear Window (1954)', 17449),\n",
    "                   (4.263182346109176, u'Band of Brothers (2001)', 4305),\n",
    "                   (4.258326830670664, u'Casablanca (1942)', 24349),\n",
    "                   (4.256934865900383, u'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)', 6525),\n",
    "                   (4.24807897901911, u\"One Flew Over the Cuckoo's Nest (1975)\", 29932),\n",
    "                   (4.247286821705426, u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 23220),\n",
    "                   (4.246001523229246, u'Third Man, The (1949)', 6565),\n",
    "                   (4.235410064157069, u'City of God (Cidade de Deus) (2002)', 12937),\n",
    "                   (4.2347902097902095, u'Lives of Others, The (Das leben der Anderen) (2006)', 5720),\n",
    "                   (4.233538107122288, u'North by Northwest (1959)', 15627),\n",
    "                   (4.2326233183856505, u'Paths of Glory (1957)', 3568),\n",
    "                   (4.227123123722136, u'Fight Club (1999)', 40106),\n",
    "                   (4.224281931146873, u'Double Indemnity (1944)', 4909),\n",
    "                   (4.224137931034483, u'12 Angry Men (1957)', 12934)],\n",
    "                  'Top 20 de películas con 500 o más puntuaciones incorrecto')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Filtro colaborativo\n",
    "En esta parte vamos a aprender a usar MLlib para realizar recomendaciones personalizadas utilizando los datos sobre las puntuaciones de las películas.\n",
    "\n",
    "Para ello vamos a usar el método de Filtro Colaborativo. Este método trata de hacer predicciones automáticas (filtro) sobre los intereses de un usuario a partir de los gustos de otros muchos usuarios (colaborativo). La suposición sobre la que se realiza el filtro colaborativo es que si a una persona A tiene la misma opinión que una persona B en un tema, es más probable que A tenga la misma opinión que B en otro tema x diferente, respecto a que tenga la opinión sobre x de una persona elegida al azar. Más información: [1][collab], [2][collab2].\n",
    "\n",
    "Para la recomendación de películas, partimos de una matriz cuyas entradas son las puntuaciones dadas a las películas por los usuarios, donde cada columna representa a un usuario y cada fila representa a una película.\n",
    "\n",
    "Como todos los usuarios no han puntuado todas las películas, tenemos entradas de la matriz con valor desconocido, razón por la que necesitamos aplicar el filtro colaborativo. Para cada usuario, tenemos las puntuaciones para un pequeño subconjunto de películas. Con el filtro colaborativo la idea es aproximar las puntuaciones de la matriz factorizándola como el producto de dos matrices: una que describe las propiedades de cada usuario y otra que describe las de cada película.\n",
    "\n",
    "Vamos a tratar de seleccionar estas matrices de tal forma que el error para los pares de usuarios/películas que conocemos sea el mínimo. Para estre proósito usamos el método de [Alternating Least Squares][als]. Este algoritmo primero inicializa aleatoriamente la matriz de usuarios y trata de optimizar la matriz de películas. Luego, mantiene la matriz de películas constante y optimiza la de usuarios. Esta alternancia entre la optimización de ambas matrices es la razón por la que se denomina así el método.\n",
    "\n",
    "[als]: https://en.wikiversity.org/wiki/Least-Squares_Method\n",
    "[collab]: https://en.wikipedia.org/?title=Collaborative_filtering\n",
    "[collab2]: http://recommender-systems.org/collaborative-filtering/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Creación del conjunto de entrenamiento\n",
    "\n",
    "Antes de usar Machine Learning, tenemos que dividir el conjunto de `ratings_df` en tres partes:\n",
    "* Un conjunto de entrenamiento (DataFrame), que usaremos para entrenar los modelos\n",
    "* Un conjunto de validación(DataFrame), que usaremos para elegir el mejor modelo\n",
    "* Un conjunto de test (DataFrame), que usaremos para conocer el rendimiento del modelo final\n",
    "\n",
    "Para dividir el DataFrame en diferentes grupos podemos usar la transformación [randomSplit()](http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) . `randomSplit()` toma un vector con los porcentages de datos que irán a cada partición y una semilla y devuelve tantos DataFrames como elementos tiene el vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.781s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 70.0 (TID 225): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[31.781s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 70.0 (TID 228): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[31.781s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 70.0 (TID 227): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[31.818s][warning][gc,alloc] Executor task launch worker for task 10.0 in stage 70.0 (TID 230): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_8 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_7 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_5 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 ERROR Executor: Exception in task 7.0 in stage 70.0 (TID 227)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/19 15:38:24 ERROR Executor: Exception in task 8.0 in stage 70.0 (TID 228)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "25/12/19 15:38:24 ERROR Executor: Exception in task 5.0 in stage 70.0 (TID 225)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "25/12/19 15:38:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 70.0 (TID 227),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/19 15:38:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 70.0 (TID 225),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "25/12/19 15:38:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 70.0 (TID 228),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "25/12/19 15:38:24 ERROR TaskSetManager: Task 8 in stage 70.0 failed 1 times; aborting job\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_11 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_1 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_4 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_6 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_2 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_10 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_9 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_3 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 WARN BlockManager: Putting block rdd_208_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/19 15:38:24 WARN BlockManager: Block rdd_208_0 could not be removed as it was not found on disk or in memory\n",
      "25/12/19 15:38:24 ERROR Executor: Exception in task 10.0 in stage 70.0 (TID 230)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/19 15:38:24 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 10.0 in stage 70.0 (TID 230),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 9.0 in stage 70.0 (TID 229) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 2.0 in stage 70.0 (TID 222) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 4.0 in stage 70.0 (TID 224) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 6.0 in stage 70.0 (TID 226) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 0.0 in stage 70.0 (TID 220) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 11.0 in stage 70.0 (TID 231) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 1.0 in stage 70.0 (TID 221) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/19 15:38:24 WARN TaskSetManager: Lost task 3.0 in stage 70.0 (TID 223) (172.19.109.126 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 70.0 failed 1 times, most recent failure: Lost task 8.0 in stage 70.0 (TID 228) (172.19.109.126 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2111/0x0000000301b2c9c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2579/0x0000000301c3e3f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/77/6t1fp7hd3bsbwjkb0pg8hbv40000gn/T/ipykernel_2898/3573934990.py\", line 12, in <module>\n",
      "    training_df.count(), validation_df.count(), test_df.count())\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1234, in count\n",
      "    return int(self._jdf.count())\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m test_df \u001b[38;5;241m=\u001b[39m split_b_20_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, validación: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, test: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m---> 12\u001b[0m       training_df\u001b[38;5;241m.\u001b[39mcount(), \u001b[43mvalidation_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, test_df\u001b[38;5;241m.\u001b[39mcount())\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m training_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m3\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/ipykernel/zmqshell.py:587\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    581\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    582\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    584\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    588\u001b[0m }\n\u001b[1;32m    590\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Utilizaremos el 60% de los datos para entrenamiento, el 20% para validación y el 20% para test\n",
    "# Utilizar randomSplit\n",
    "seed = 1800009193\n",
    "(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6, 0.2, 0.2], seed)\n",
    "\n",
    "# Vamos a cachear los tres DataFrames ¡¡¡CUIDADO LABORATORIO!!!!!!\n",
    "training_df = split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print('Train: {0}, validación: {1}, test: {2}\\n'.format(\n",
    "      training_df.count(), validation_df.count(), test_df.count())\n",
    "    )\n",
    "\n",
    "training_df.show(3)\n",
    "validation_df.show(3)\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Creación del conjunto de entrenamiento (2a)\n",
    "print (\"Si los primeros tres test fallan pero los números son similares, no hay problema\")\n",
    "Test.assertEquals(training_df.count(), 12002750, \"Conteo de training_df incorrecto. Se esperaban 12002750\")\n",
    "Test.assertEquals(validation_df.count(), 3999777, \"Conteo de validation_df incorrecto. Se esperaban 3999777\")\n",
    "Test.assertEquals(test_df.count(), 3997736, \"Conteo de test_df incorrecto. Se esperaban 3997736\")\n",
    "\n",
    "Test.assertEquals(training_df.filter((ratings_df.userId == 1) & (ratings_df.movieId == 5952) & (ratings_df.rating == 5.0)).count(), 1)\n",
    "Test.assertEquals(training_df.filter((ratings_df.userId == 1) & (ratings_df.movieId == 1193) & (ratings_df.rating == 3.5)).count(), 1)\n",
    "Test.assertEquals(training_df.filter((ratings_df.userId == 1) & (ratings_df.movieId == 1196) & (ratings_df.rating == 4.5)).count(), 1)\n",
    "\n",
    "Test.assertEquals(validation_df.filter((ratings_df.userId == 1) & (ratings_df.movieId == 296) & (ratings_df.rating == 4.0)).count(), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de dividir el DataFrame, el train debe tener unas 12 millones de instancias y la validación y el test unas 4 millones. (El número exacto de entradas puede variar ligeramente por la aleatoriedad de `randomSplit()`.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Alternating Least Squares\n",
    "\n",
    "En esta parte vamos a usar la implementación del algoritmo en MLlib [ALS](http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS). ALS toma un conjunto de entrenamiento (DatAFrame) y varios parámetros que controlan la creación del modelo. Para determinar los mejores valores para los parámetros utilizaremos ALS para entrenar varios modelos y elegiremos aquel con el mejor comportamiento para el resto de la práctica.\n",
    "\n",
    "Para determinar el mejor modelo usaremos la partición de validación de la siguiente forma:\n",
    "**NOTA: Idealmente, podríamos utilizar `CrossValidator` o `TrainValidationSplit`de MLlib para la estimación. Sin embargo tiene algunos problemas con ALS como explicamos después.**\n",
    "\n",
    "1. Seleccionamos un conjunto de parámetros para probar el modelo. El parámetro más importante en ALS es *rank*, que determina el número de columnas en la matriz de usuarios o el número de filas en la matriz de películas. En general, un rank menor conllevará un mayor error en training, pero un rank muy alto puede llevarnos a sobreentrenar y obtener malas predicciones en validación/test. Usaremos los valores de rank = {4, 8, 12} con el DataFrame `training_df`\n",
    "\n",
    "2. Establecemos los parámetros necesarios para ejecutar ALS:\n",
    "    * La columna \"User\" será la columna `userId` de nuestro DataFrame.\n",
    "    * La columna \"Item\" será la columna `movieId` de nuestro DataFrame.\n",
    "    * La columna \"Rating\" será la columna `rating` de nuestro DataFrame.\n",
    "    * La columna \"Prediction\" se llamará `prediction` en nuestro DataFrame generado.\n",
    "    * Utilizaremos el parámetro de regularización con valor 0.1 (este podría ser otro parámetro a ajustar como rank).\n",
    "\n",
    "**Nota**: Documentación de [ALS] [ALS](http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS)\n",
    "\n",
    "\n",
    "3. Crearemos varios modelos usando [ALS.fit()](http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS.fit), uno para cada valor de rank. Utilizar el conjunto de entrenamiento (`training_df`).\n",
    "\n",
    "4. Con cada modelo obtenido, usaremos el método `transform()` para obtener la predicción sobre el conjunto de validación  (`validation_df`) en un nuevo DataFrame con la predicción en una nueva columna llamada \"prediction\".\n",
    "\n",
    "5. Comprobaremos el error obtenido.\n",
    "\n",
    "6. Nos quedaremos con el modelo que obtenga el menor error en validación.\n",
    "\n",
    "#### ¿Por qué usar nuestra propia validación y no la de MLlib?\n",
    "\n",
    "Un tema importante del filtro colaborativo es como dar puntuaciones a los nuevos usuarios (usuarios que no han puesto ninguna puntuación). Algunos sistemas dan unas predicciones por defecto y otros no predicen nada para nuevos usuarios. Este segundo es el caso de Spark ALS, produce NaN's para las recomendaciones a usuarios nuevos. \n",
    "\n",
    "Sin entrar en mucho detalle, para usar  [CrossValidator](http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) debemos usar un evaluador como RMSE que no soporta NaN's y a nada que haya un NaN el resultado serán NaN y la elección de parámetros no tendrá sentido. El problema es que al hacer el particionamiento puede haber usuarios sin puntuaciones en training, por lo que sus puntuaciones predichas en validación serán NaN.\n",
    "\n",
    "Este es un problema con diferentes soluciones. En nuestro caso, simplemente eliminaremos los NaNs antes de calcular el error cuadrático medio (RMSE), y esta es la razón por la que lo hacemos manualmente.\n",
    "\n",
    "**NOTA: el próximo código tardará un poco en ejecutarse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m als\u001b[38;5;241m.\u001b[39msetRank(rank)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo con estos parámetros\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m als\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtraining_df\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Ejecutamos el modelo para predecir los valores usando transform() en validación (validation_df)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m predict_df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(validation_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Inicializamos el Estimator ALS\n",
    "als = ALS()\n",
    "\n",
    "# Establecemos los parámetros para ALS\n",
    "# Establecer las columnas de usuario, item, rating y predicción, setUserCol, setItemCol, setRatingCol, setPredictionCol\n",
    "als.setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\").setPredictionCol(\"prediction\").setRegParam(0.1)\n",
    "\n",
    "# Importamos el Evaluator para el conjunto de validación\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Creamos un evaluator RMSE usando la etiqueta que es rating y la predicción que es prediction\n",
    "reg_eval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "tolerance = 0.03\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "models = [0, 0, 0]\n",
    "err = 0\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "\n",
    "for rank in ranks:\n",
    "    # Establecemos el valor de rank con setRank para ALS\n",
    "    als.setRank(rank)\n",
    "    \n",
    "    # Entrenamos el modelo con estos parámetros\n",
    "    model = als.fit(training_df)\n",
    "    \n",
    "    # Ejecutamos el modelo para predecir los valores usando transform() en validación (validation_df)\n",
    "    predict_df = model.transform(validation_df)\n",
    "\n",
    "    # Eliminamos los valores NaN\n",
    "    predicted_ratings_df = predict_df.na.drop(subset=[\"prediction\"])\n",
    "\n",
    "    # Ejecutamos el evaluador RMSE creado previamente, reg_eval.evaluate, sobre el DataFrame predicted_ratings_df\n",
    "    error = reg_eval.evaluate(predicted_ratings_df)\n",
    "    errors[err] = error\n",
    "    models[err] = model\n",
    "    print('Para el rank %s el RMSE es %s' % (rank, error))\n",
    "    \n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = err\n",
    "        \n",
    "    err += 1\n",
    "\n",
    "als.setRank(ranks[best_rank])\n",
    "print('El mejor modelo ha sido entrenado con rank %s' % ranks[best_rank])\n",
    "my_model = models[best_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertEquals(__builtin__.round(min_error, 2), 0.82, \"Valor no esperado para el mejor RMSE. Debía ser 0.82 (redondeado). Obtenido {0}\".format(__builtin__.round(min_error, 2)))\n",
    "Test.assertEquals(ranks[best_rank], 8, \"Valor no esperado para el mejor rank. Debía ser 8. Obtenido {0}\".format(ranks[best_rank]))\n",
    "Test.assertEqualsHashed(als.getItemCol(), \"18f0e2357f8829fe809b2d95bc1753000dd925a6\", \"Columna de item en ALS incorrecta {0}.\".format(als.getItemCol()))\n",
    "Test.assertEqualsHashed(als.getUserCol(), \"db36668fa9a19fde5c9676518f9e86c17cabf65a\", \"Columna de user en ALS incorrecta {0}.\".format(als.getUserCol()))\n",
    "Test.assertEqualsHashed(als.getRatingCol(), \"3c2d687ef032e625aa4a2b1cfca9751d2080322c\", \"Columna de rating en ALS incorrecta {0}.\".format(als.getRatingCol()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Testeando el modelo\n",
    "\n",
    "Para saber lo bueno que es realmente el modelo obtenido debemos utilizar el conjunto de test (`test_df`), que no ha sido utilizado ni para el entrenamiento ni para la elección de parámetros `test_df` dataset. Utilizaremos el modelo obtenido con el mejor rank (`best_rank`) almacenado en `my_model`para realizar las predicciones sobre el conjunto de test y obtendremos el error sobre este conjunto usando la raíz error cuadrático medio RMSE.\n",
    "\n",
    "Debes seguir los siguientes pasos:\n",
    "* Utiliza el método `transform()` del modelo `my_model` para predecir las puntuaciones en el conjunto de `test_df`. Obtendremos un nuevo DataFrame `predict_df`.\n",
    "* Filtramos los valores NaN obtenidos en la predicción. Utilizar el código incluido.\n",
    "* Usar el evaluador de RMSE `reg_eval` para obtener el error en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Predecir los valores para test_df\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predict_df \u001b[38;5;241m=\u001b[39m \u001b[43mmy_model\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(test_df)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Eliminamos los valores NaN\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predicted_test_df \u001b[38;5;241m=\u001b[39m predict_df\u001b[38;5;241m.\u001b[39mna\u001b[38;5;241m.\u001b[39mdrop(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Predecir los valores para test_df\n",
    "predict_df = my_model.transform(test_df)\n",
    "\n",
    "# Eliminamos los valores NaN\n",
    "predicted_test_df = predict_df.na.drop(subset=[\"prediction\"])\n",
    "\n",
    "# Ejecutamos el evaluador RMSE, reg_eval, sobre predicted_test_df\n",
    "test_RMSE = reg_eval.evaluate(predicted_test_df)\n",
    "print('El modelo ha obtenido un RMSE en test de {0}'.format(test_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertTrue(__builtin__.abs(test_RMSE - 0.809624038485) < tolerance, 'incorrect test_RMSE: {0:.11f}'.format(test_RMSE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Comparando el modelo obtenido \n",
    "\n",
    "Atendiendo solo al RMSE obtenido es difícil saber si el modelo es de calidad o no. Sin embargo, podemos comparar este error frente a lo que sería predecir para todas las películas y usuarios la media de todas las puntuaciones en el entrenamiento. Es evidente que nuestro modelo debe comportarse mejor que esta predicción \"tonta\".\n",
    "\n",
    "Pasos a seguir:\n",
    "* Usar el DataFrame `training_df` para calcular la media de todas las puntuaciones (usar agg o groupBy() + avg).\n",
    "* Utilizar la puntuación media junto con el DataFrame `test_df` para crear otro DataFrame (`test_for_avg_df`) con una columna `prediction` con el valor de puntuación medio para todas las filas. **Nota**: Utilizar la función `lit()` para crear la columna con dicho valor. Utiliza la transformación `withColumn`.\n",
    "* Utiliza el evaluador `reg_eval` para obtener el error sobre el nuevo DataFrame `test_for_avg_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la media de todas las puntuaciones en training_df\n",
    "avg_rating_df = training_df.agg(avg('rating').alias('avg_rating'))\n",
    "\n",
    "# Extrae el valor medio de las puntuaciones (fila 0, columna 0)\n",
    "training_avg_rating = avg_rating_df.first()['avg_rating']\n",
    "print('El valor medio de las puntuaciones en el conjunto de entrenamiento es {0}'.format(training_avg_rating))\n",
    "\n",
    "# Añade una columna con el valor medio de las puntuaciones\n",
    "test_for_avg_df = test_df.withColumn('prediction', lit(training_avg_rating))\n",
    "\n",
    "# Ejecuta el evaluador RMSE, reg_eval, sobre el DataFrame test_for_avg_df\n",
    "test_avg_RMSE = reg_eval.evaluate(test_for_avg_df)\n",
    "print(\"El RMSE de la predicción constante para todos es {0}\".format(test_avg_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertTrue(__builtin__.abs(training_avg_rating - 3.52578917331) < 0.001\n",
    "                'Valor incorrecto de training_avg_rating (expected 3.52578917331): {0:.11f}'.format(training_avg_rating))\n",
    "Test.assertTrue(__builtin__.abs(test_avg_RMSE - 1.05190953037) < 0.001,\n",
    "                'Valor incorrecto de test_avg_RMSE (expected 1.0519743756): {0:.11f}'.format(test_avg_RMSE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ya sabemos cómo predecir puntuaciones para las películas!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Predicciones para ti\n",
    "En esta última parte del ejercicio vamos a predecir las películas que habría que recomendarte a ti mismo. Para ello, primero necesitamos saber tus preferencias.\n",
    "\n",
    "Vamos a crear un DataFrame con tus preferencias denominado `ratings_df`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3a) Tus preferencias**\n",
    "\n",
    "Para ayudarte de la hora de establecer tus preferencias, a continuación puedes obtener una lista con los nombres y códigos de las 50 películas con mejor puntuación del DataFrame `movies_with_500_ratings_or_more`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Películas mejor puntuadas:')\n",
    "print('(ID de la película, número de puntuaciones, puntuación media, título de la película)')\n",
    "movies_with_500_ratings_or_more.orderBy(movies_with_500_ratings_or_more['average'].desc()).show(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ID de usuario 0 no está asignado, por lo que lo usaremos para tus puntuaciones. La variable `my_user_ID` tiene el valor 0 asignado. Utilizándolo, crea un nuevo DataFrame llamado `my_ratings_df` con tus puntuaciones para al menos 10 películas. Cada entrada tiene que estar en el siguiente formato:  `(my_user_id, movieID, rating)`.  \n",
    "\n",
    "Como en el dataset original, las puntuaciones deben estar entre 1 y 5 (incluidos ambos). Si no has visto al menos 10 de esas películas puedes mostrar más cambiando el parámetro pasado a `take()` hasta que encuentres 10 películas que hayas visto (o sino puedes establecer la puntuación que crees que tendrían las películas para ti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "my_user_id = 0\n",
    "\n",
    "# Ten en cuenta que el ID de las películas es el último número en cada línea\n",
    "# Un error común es utilizar el número de puntuación como ID de película, ¡ten cuidado!\n",
    "my_rated_movies = [ \n",
    "    (my_user_id, 260, 5.0),   # Star Wars: Episode IV - A New Hope (1977)\n",
    "    (my_user_id, 1196, 5.0),  # Star Wars: Episode V - The Empire Strikes Back (1980)\n",
    "    (my_user_id, 1210, 4.5),  # Star Wars: Episode VI - Return of the Jedi (1983)\n",
    "    (my_user_id, 296, 4.5),   # Pulp Fiction (1994)\n",
    "    (my_user_id, 318, 5.0),   # The Shawshank Redemption (1994)\n",
    "    (my_user_id, 356, 4.5),   # Forrest Gump (1994)\n",
    "    (my_user_id, 110, 4.0),   # Braveheart (1995)\n",
    "    (my_user_id, 1, 4.0),     # Toy Story (1995)\n",
    "    (my_user_id, 589, 4.5),   # Terminator 2: Judgment Day (1991)\n",
    "    (my_user_id, 593, 4.5),   # The Silence of the Lambs (1991)\n",
    "    (my_user_id, 527, 4.5),   # Schindler's List (1993)\n",
    "    (my_user_id, 480, 4.0),   # Jurassic Park (1993)\n",
    "]\n",
    "\n",
    "# El formato de cada línea debe ser (my_user_id, movie ID, tu puntuación)\n",
    "# Por ejemplo, parala película \"Star Wars: Episode IV - A New Hope (1977)\" cinco estrellas, debería añadir la siguiente línea\n",
    "#   (my_user_id, 260, 5)\n",
    "my_ratings_df = spark.createDataFrame(my_rated_movies, ratings_df_schema)\n",
    "\n",
    "print('Mis puntuaciones son:')\n",
    "my_ratings_df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Unión de tus puntuaciones con el conjunto de entrenamiento\n",
    "\n",
    "Para poder obtener nuevas predicciones para ti, debemos incluir las puntuaciones previas en el conjunto de entrenamiento. Utiliza para ello la transformación [unionAll()](http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unionAll); utiliza `unionAll()` para crear un nuevo conjunto de entrenamiento con tus puntuaciones (`my_ratings_df`) y el conjunto de train `training_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza unionAll() para unir my_ratings_df con training_df\n",
    "training_with_my_ratings_df = training_df.unionAll(my_ratings_df)\n",
    "\n",
    "print ('El conjunto de entrenamiento tiene ahora %s más filas que el original' %\n",
    "       (training_with_my_ratings_df.count() - training_df.count()))\n",
    "assert (training_with_my_ratings_df.count() - training_df.count()) == my_ratings_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Entrenamiento del modelo con tus puntuaciones\n",
    "\n",
    "Ahora vamos a entrenar un nuevo modelo considerando tus puntuaciones junto con las que ya teníamos en el training original. Utilizaremos los mismos parámetros que en las partes (2b) y (2c). **Recuerda incluir TODOS los parámetros**.\n",
    "\n",
    "**NOTA:** Esta ejecución tardará un poco, paciencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establece los parámetros para ALS: \n",
    "#    regularización, columna de items, usuarios y puntuaciones y el rank (el mejor obtenido)\n",
    "als.setRegParam(0.1) \\\n",
    "    .setItemCol(\"movieId\") \\\n",
    "    .setUserCol(\"userId\") \\\n",
    "    .setRatingCol(\"rating\") \\\n",
    "    .setPredictionCol(\"prediction\") \\\n",
    "    .setRank(ranks[best_rank])\n",
    "\n",
    "# Entrenar el modelo con los parámetros establecidos, es decir, utilizar fit() con el nuevo training set: training_with_my_ratings_df\n",
    "my_ratings_model = als.fit(training_with_my_ratings_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Comprobar el error RMSE del nuevo modelo\n",
    "\n",
    "Vamos a calcular el RMSE obtenido para el nuevo modelo para el conjunto de test.\n",
    "* Utiliza el modelo (`transform()`) para obtener las predicciones para el conjunto de test `test_df`\n",
    "* Después, utiliza `reg_eval` (el evaluador) para calcular el RMSE (método `evaluate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_ratings_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_predict_df \u001b[38;5;241m=\u001b[39m \u001b[43mmy_ratings_model\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(test_df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Filtramos los valores NaN\u001b[39;00m\n\u001b[1;32m      4\u001b[0m predicted_test_my_ratings_df \u001b[38;5;241m=\u001b[39m my_predict_df\u001b[38;5;241m.\u001b[39mna\u001b[38;5;241m.\u001b[39mdrop(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_ratings_model' is not defined"
     ]
    }
   ],
   "source": [
    "my_predict_df = my_ratings_model.transform(test_df)\n",
    "\n",
    "# Filtramos los valores NaN\n",
    "predicted_test_my_ratings_df = my_predict_df.na.drop(subset=[\"prediction\"])\n",
    "\n",
    "# Obtener el error usando reg_eval y predicted_test_my_ratings_df\n",
    "test_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\n",
    "print('El modelo tiene un RMSE en el conjunto de test de {0}'.format(test_RMSE_my_ratings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Predicción de nuevas puntuaciones para ti\n",
    "\n",
    "Hasta ahora lo único que hemos hecho es calcular el error del modelo. Ahora, lo que vamos a hacer es predecir que puntuaciones le habrías dado a las películas que todavía no has puntuado.\n",
    "\n",
    "Debemos seguir los siguientes pasos:\n",
    "* Filtrar las películas que has puntuado manualmente. Utilizaremos la variable `my_rated_movie_ids` para guardar los IDs de las películas puntuadas y almacenaremos las no puntuadas en un DataFrame `not_rated_df`.\n",
    "\n",
    "   **Nota**: La función [Column.isin()](http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.Column.isin)\n",
    "   y el operador lógico \"no\" `~` pueden ayudar en esta operación. Aquí viene un ejemplo con `isin()`:\n",
    "\n",
    "```\n",
    "    > df1 = spark.createDataFrame([(\"Jim\", 10), (\"Julie\", 9), (\"Abdul\", 20), (\"Mireille\", 19)], [\"name\", \"age\"])\n",
    "    > df1.show()\n",
    "    +--------+---+\n",
    "    |    name|age|\n",
    "    +--------+---+\n",
    "    |     Jim| 10|\n",
    "    |   Julie|  9|\n",
    "    |   Abdul| 20|\n",
    "    |Mireille| 19|\n",
    "    +--------+---+\n",
    "\n",
    "    > names_to_delete = [\"Julie\", \"Abdul\"] # esto es  una lita de Python\n",
    "    > df2 = df1.filter(~ df1[\"name\"].isin(names_to_delete)) # \"NOT IN\"\n",
    "    > df2.show()\n",
    "    +--------+---+\n",
    "    |    name|age|\n",
    "    +--------+---+\n",
    "    |     Jim| 10|\n",
    "    |Mireille| 19|\n",
    "    +--------+---+\n",
    "```\n",
    "\n",
    "* Transformar `not_rated_df` en `my_unrated_movies_df` siguiendo los siguientes pasos:\n",
    "    - renombrar la columna \"ID\" por \"movieId\" utilizando `withColumnRenamed()`\n",
    "    - añadir una columna \"userId\" con el valor que tiene la variable `my_user_id` definida anteriormente. Utiliza para ello el método `withColumn()` junto con la función `lit()` para añadir una columna con el mismo valor en todas las filas.\n",
    "\n",
    "* Crear el DataFrame `predicted_ratings_df` aplicando el modelo aprendindo `my_ratings_model` al DataFrame `my_unrated_movies_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista con los IDs de las películas ya puntuadas\n",
    "my_rated_movie_ids = [x[1] for x in my_rated_movies] # simplemente cogemos el segundo valor de las tuplas\n",
    "\n",
    "# Filtrar las películas ya puntuadas - utiliza my_rated_movie_ids con el método isin sobre la columna \"ID\" - ver ejemplo anterior\n",
    "not_rated_df = movies_df.filter(~ movies_df[\"ID\"].isin(my_rated_movie_ids))\n",
    "\n",
    "# Renombrar la columna \"ID\" por \"movieId\" y añadir la columna \"userId\" con my_user_id usando la función lit()\n",
    "my_unrated_movies_df = not_rated_df.withColumnRenamed(\"ID\", \"movieId\") \\\n",
    "                                   .withColumn(\"userId\", lit(my_user_id))\n",
    "\n",
    "# Utilizar el modelo my_ratings_model para predecir las puntuaciones dadas a las películas que no han sido puntuadas manualmente\n",
    "raw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n",
    "\n",
    "# Filtramos los NaN\n",
    "predicted_ratings_df = raw_predicted_ratings_df.na.drop(subset=[\"prediction\"]) \\\n",
    "                                                 .select(\"userId\", \"movieId\", \"prediction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Predicción final de las películas recomendadas para ti\n",
    "\n",
    "Ya hemos predecido las puntuaciones, ahora podemos mostrar las 25 películas con mejores puntuaciones que serían las recomendadas.\n",
    "\n",
    "Pasos a seguir:\n",
    "* Unir el DataFrame `predicted_ratings_df` con `movie_names_with_avg_ratings_df` para obtener el conteo de puntuaciones para cada película\n",
    "* Ordenar el DataFrame resultante (`predicted_with_counts_df`) por la puntuación (descendente) y eliminar las películas con 75 puntuaciones o menos\n",
    "* Imprimir el top 25 de las películas restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necesario para que funcione el join según la versión de spark\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza un join() para unir predicted_ratings_df con movie_names_with_avg_ratings_df (campo movieId)\n",
    "predicted_with_counts_df = predicted_ratings_df.join(\n",
    "    movie_names_with_avg_ratings_df.select('movieId', 'count', 'title'),\n",
    "    on='movieId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Utiliza un filter para filtrar las películas con al menos de 75 puntuaciones, aprovecha para ordenar el DataFrame por la columna prediction de manera descendente\n",
    "predicted_highest_rated_movies_df = (\n",
    "    predicted_with_counts_df\n",
    "    .filter(col('count') >= 75)\n",
    "    .orderBy(desc('prediction'))\n",
    "    .limit(25)\n",
    ")\n",
    "\n",
    "# Mostrar el top25 de las películas con show()\n",
    "print ('El top 25 de películas recomendadas para ti es (películas con al menos 75 puntuaciones):')\n",
    "predicted_highest_rated_movies_df.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "name": "cs110_lab2_als_prediction",
  "notebookId": 3391522265018542
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
