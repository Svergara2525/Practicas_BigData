{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grado en ciencia de datos - Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2.1 - Introducción a SparkSQL\n",
    "\n",
    "En esta práctica veremos las operaciones básicas para trabajar con los DataFrames de Spark. Esta primera práctica es una guía de todas las operaciones que se peuden realizar en SparkSQL con DataFrames. Prueba a cambiar los valores establecidos para comprobar su funcionamiento.\n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializar el `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/12 13:05:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/tmp/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API de SparkSQL\n",
    "<http://spark.apache.org/docs/latest/api/python/pyspark.sql.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un DataFrame\n",
    "Podemos crear un DataFrame de 3 diferentes formas:\n",
    "\n",
    "1. Infiriendo el esquema automáticamente a partir de los datos\n",
    "1. Infiriendo el esquema automáticamente a partir de los metadatos\n",
    "1. Definiendo explícitamente el esquema\n",
    "\n",
    "A su vez, como con los RDDs podemos crearlo a partir de dos fuentes:\n",
    "\n",
    "1. Cargando un conjunto de datos almacenado en un medio externo\n",
    "2. Distribuyendo una colección de objetos existente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia del esquema a partir de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin nombre para las columnas: [Row(_1='Alice', _2=1), Row(_1='Bob', _2=4), Row(_1='Juan', _2=10), Row(_1='Pepe', _2=25), Row(_1='Panchito', _2=15)]\n",
      "Con nombre para las columnas: [Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n"
     ]
    }
   ],
   "source": [
    "tuplas = [('Alice', 1), ('Bob', 4), ('Juan', 10), ('Pepe', 25), ('Panchito', 15)]\n",
    "\n",
    "print(\"Sin nombre para las columnas: \" + str(spark.createDataFrame(tuplas).collect()))\n",
    "print(\"Con nombre para las columnas: \" + str(spark.createDataFrame(tuplas, ['name', 'age']).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin nombre para las columnas: [Row(_1='Alice', _2=1), Row(_1='Bob', _2=4), Row(_1='Juan', _2=10), Row(_1='Pepe', _2=25), Row(_1='Panchito', _2=15)]\n",
      "Con nombre para las columnas: [Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(tuplas)\n",
    "\n",
    "print(\"Sin nombre para las columnas: \" + str(spark.createDataFrame(rdd).collect()))\n",
    "print(\"Con nombre para las columnas: \" + str(spark.createDataFrame(rdd, ['name', 'age']).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1),\n",
       " Row(name='Bob', age=4),\n",
       " Row(name='Juan', age=10),\n",
       " Row(name='Pepe', age=25),\n",
       " Row(name='Panchito', age=15)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Person = Row('name', 'age') # Creamos una Row con los índices para poder crear filas con datos utilizándola\n",
    "person = rdd.map(lambda r: Person(*r)) # con *r lo que hacemos es pasar la tupla como parámetro a la fila y crearla directamente con dichos datos\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Panda obtenido de un DataFrame Spark:\n",
      "       name  age\n",
      "0     Alice    1\n",
      "1       Bob    4\n",
      "2      Juan   10\n",
      "3      Pepe   25\n",
      "4  Panchito   15\n",
      "DataFrame de Spark creado a partir del Panda:\n",
      "[Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n"
     ]
    }
   ],
   "source": [
    "pandaDF = df2.toPandas()\n",
    "print(\"DataFrame de Panda obtenido de un DataFrame Spark:\")\n",
    "print(pandaDF)\n",
    "print(\"DataFrame de Spark creado a partir del Panda:\")\n",
    "print(spark.createDataFrame(pandaDF).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1),\n",
       " Row(name='Bob', age=4),\n",
       " Row(name='Juan', age=10),\n",
       " Row(name='Pepe', age=25),\n",
       " Row(name='Panchito', age=15)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(a='Alice', b=1), Row(a='Bob', b=4), Row(a='Juan', b=10), Row(a='Pepe', b=25), Row(a='Panchito', b=15)]\n",
      "[Row(value=1), Row(value=4), Row(value=10), Row(value=25), Row(value=15)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.createDataFrame(rdd, \"a: string, b: int\").collect())\n",
    "\n",
    "rdd2 = rdd.map(lambda row: row[1])\n",
    "print(spark.createDataFrame(rdd2, \"int\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "data = [\n",
    "    Row(\n",
    "        name=\"Alex\",\n",
    "        num_pets=3,\n",
    "        paid_in_full=True,\n",
    "        preferences={\n",
    "            \"preferred_vet\": \"Dr. Smith\",\n",
    "            \"preferred_appointment_day\": \"Monday\"\n",
    "        },\n",
    "        registered_on=datetime.datetime(2015, 1, 1, 12,0),\n",
    "        visits=[\n",
    "            datetime.datetime(2015, 2, 1, 11, 0),\n",
    "            datetime.datetime(2015, 2, 2, 10, 45),\n",
    "        ],\n",
    "    ), \n",
    "    Row(\n",
    "        name=\"Charlie\",\n",
    "        num_pets=1,\n",
    "        paid_in_full=True,\n",
    "        preferences={},\n",
    "        registered_on=datetime.datetime(2016, 5, 1, 12,0),\n",
    "        visits=[],\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    "    Row(\n",
    "        name=\"Beth\",\n",
    "        num_pets=2,\n",
    "        paid_in_full=False,\n",
    "        preferences={\n",
    "            \"preferred_vet\": \"Dr. Travis\",\n",
    "        },\n",
    "        registered_on=datetime.datetime(2013, 1, 1, 12,0),\n",
    "        visits=[\n",
    "            datetime.datetime(2015, 1, 15, 12, 15),\n",
    "            datetime.datetime(2015, 2, 1, 11, 15),\n",
    "        ],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia del esquema a partir de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD: Esquema inferido a partir de la primera fila.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: long (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: timestamp (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n",
      "RDD: Esquema inferido de un sampling aleatorio.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: long (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: timestamp (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos un RDD con los datos de ejemplo\n",
    "dataRDD = sc.parallelize(data)\n",
    "\n",
    "# Creamos un DataFrame a partir del RDD, infiriendo el esquema a partir de la primera Row\n",
    "print(\"RDD: Esquema inferido a partir de la primera fila.\")\n",
    "\n",
    "# Por defecto samplingRatio=None\n",
    "dataDF = spark.createDataFrame(dataRDD, samplingRatio=None)\n",
    "dataDF.printSchema()\n",
    "\n",
    "# Creamos un DataFrame a partir del RDD, infiriendo el esquema a partir de un sampling de las Rows\n",
    "print(\"RDD: Esquema inferido de un sampling aleatorio.\")\n",
    "dataDF = spark.createDataFrame(dataRDD, samplingRatio=0.6)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD: Esquema espcificado explícitamente.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: integer (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: date (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, BooleanType, DateType, \\\n",
    "    IntegerType, MapType, StringType, TimestampType, StructField, StructType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"num_pets\", IntegerType(), True),\n",
    "        StructField(\"paid_in_full\", BooleanType(), True),\n",
    "        StructField(\"preferences\", MapType(StringType(), StringType(), True), True),\n",
    "        StructField(\"registered_on\", DateType(), True),\n",
    "        StructField(\"visits\", ArrayType(TimestampType(), True), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear un DataFrame a partir de un RDD, especificando el esquema\n",
    "print(\"RDD: Esquema espcificado explícitamente.\")\n",
    "dataDF = spark.createDataFrame(dataRDD, schema)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del DataFrame desde un fichero JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: Esquema inferido a partir de todas las filas.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: long (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: struct (nullable = true)\n",
      " |    |-- preferred_appointment_day: string (nullable = true)\n",
      " |    |-- preferred_vet: string (nullable = true)\n",
      " |-- registered_on: string (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un fichero JSON, infiriendo el esquema a partir de todas las filas\n",
    "print(\"JSON: Esquema inferido a partir de todas las filas.\")\n",
    "dataDF = spark.read.option(\"samplingRatio\", 1.0).json(\"datos/data.json\")\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: Esquema especificado explícitamente.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: integer (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: date (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un fichero JSON, especificando el esquema\n",
    "print(\"JSON: Esquema especificado explícitamente.\")\n",
    "dataDF = spark.read.json(\"datos/data.json\", schema)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceso a las columnas de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'age'>\n",
      "Column<'age'>\n",
      "Column<'(age + 1) AS `cumpleaños`'>\n"
     ]
    }
   ],
   "source": [
    "print(df2.age)\n",
    "print(df2[\"age\"])\n",
    "print((df2.age + 1).alias(\"cumpleaños\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay dos tipos de operaciones sobre DataFrames, como los RDDs:\n",
    "1. Transformaciones: Crean un nuevo DataFrame a partir de otro **(EVALUACIÓN VAGA)** hasta que no se ejecuta una acción no se realiza la transformación\n",
    "2. Acciones: Utilizan el DataFrame para lograr un resultado, es recibido por el driver o escrita en disco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones sobre DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones básicas\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*select(*\\**cols)* | Devuelve un nuevo DataFrame proyectando una serie de expresiones que pueden ser nombres de columnas o expresiones de tipo Column. Si se utiliza \"\\*\", todas las columnas del DataFrame se proyectan al nuevo DataFrame\n",
    "*selectExpr(*\\**expr)* | Variante de select que admite expresiones SQL\n",
    "*filter(condition) / where(condition)* | Filtra las filas usando la condición especificada\n",
    "*orderBy(*\\**cols, ascending)* | Devuelve un nuevo DataFrame ordenado por las columnas especificadas. Por defecto en orden ascendente\n",
    "*sort(*\\**cols, *\\*\\**kwargs)* | Devuelve un DataFrame ordenado por las columnas especificadas. La condición debe ser tipo Column o expresión SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de ejemplo\n",
    "tuplas = [('Alice', 1), ('Bob', 4), ('Juan', 10), ('Pepe', 25), ('Panchito', 15)]\n",
    "df = spark.createDataFrame(tuplas, ['name', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `select(*cols)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n",
      "[Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n",
      "[Row(name='Alice', age=11), Row(name='Bob', age=14), Row(name='Juan', age=20), Row(name='Pepe', age=35), Row(name='Panchito', age=25)]\n"
     ]
    }
   ],
   "source": [
    "print(df.select('*').collect())\n",
    "\n",
    "print(df.select('name', 'age').collect())\n",
    "\n",
    "print(df.select(df.name, (df.age + 10).alias('age')).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selectExpr(*expr)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row((age * 2)=2, abs(age)=1),\n",
       " Row((age * 2)=8, abs(age)=4),\n",
       " Row((age * 2)=20, abs(age)=10),\n",
       " Row((age * 2)=50, abs(age)=25),\n",
       " Row((age * 2)=30, abs(age)=15)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"age * 2\", \"abs(age)\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `filter(condition) = where(condition)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo POO, edad > 18: [Row(name='Pepe', age=25)]\n",
      "Modo POO, edad == 1: [Row(name='Alice', age=1)]\n",
      "Modo SQL, edad > 18: [Row(name='Pepe', age=25)]\n",
      "Modo SQL, edad = 1: [Row(name='Alice', age=1)]\n"
     ]
    }
   ],
   "source": [
    "# Modo programación orientada a objetos (POO)\n",
    "print(\"Modo POO, edad > 18: \" + str( df.filter(df.age > 18).collect() ))\n",
    "\n",
    "print(\"Modo POO, edad == 1: \" + str( df.where(df.age == 1).collect() ))\n",
    "\n",
    "# Modo SQL\n",
    "print(\"Modo SQL, edad > 18: \" + str( df.filter(\"age > 18\").collect() ))\n",
    "\n",
    "print(\"Modo SQL, edad = 1: \" + str( df.where(\"age = 1\").collect() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `orderBy(*cols, ascending)` y `sort(*cols, **kwargs)`\n",
    "\n",
    "Diferentes argumentos, pero el resultado es el mismo. En ambas, se reciben primero las columnas por las que ordenar el DataFrame y después la forma en la que se quiere ordenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordenar por edad descendente:\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "Ordenar por edad ascendente:\n",
      "[Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Panchito', age=15), Row(name='Pepe', age=25)]\n",
      "Ordenar por edad descendente y nombre ascendente:\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ordenar por edad descendente:\")\n",
    "print(df.sort(df.age.desc()).collect())\n",
    "print(df.sort(\"age\", ascending=False).collect())\n",
    "print(df.orderBy(df.age.desc()).collect())\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Ordenar por edad ascendente:\")\n",
    "print(df.sort(asc(\"age\")).collect())\n",
    "\n",
    "print(\"Ordenar por edad descendente y nombre ascendente:\")\n",
    "print(df.orderBy(desc(\"age\"), \"name\").collect())\n",
    "print(df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones adicionales\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*distinct()* | Devuelve un nuevo DataFrame con las filas únicas del original\n",
    "*dropDuplicates(*\\**cols)* | Devuelve un nuevo DataFrame sin filas duplicadas considerando las columnas especificadas\n",
    "*withColumn(colName, col)* | Devuelve un nuevo DataFrame añadiendo una nueva columna o reemplazando la columna existente con el mismo nombre\n",
    "*withColumnRenamed(existing, new)* | Devuelve un DataFrame renombrando una columna existente\n",
    "*drop(col)* | Devuelve un nuevo DataFrame con la columna especificada eliminada\n",
    "*limit(num)* | Limita el número de filas obtenidas como resultado\n",
    "*cache()* | Mantiene el DataFrame almacenado en memoria para ser reusado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `distinct()` y `dropDuplicates(*cols)`\n",
    "La diferencia está en que en `dropDuplicates` podemos especificar los campos por los cuales decidimos que dos filas están repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "distinct(): \n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "dropDuplicates(): \n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "dropDuplicates('name', 'height')\n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df2 = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80)]).toDF()\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "df2.show()\n",
    "\n",
    "print(\"distinct(): \")\n",
    "df2.distinct().show()\n",
    "\n",
    "print(\"dropDuplicates(): \")\n",
    "df2.dropDuplicates().show() \n",
    "\n",
    "print(\"dropDuplicates('name', 'height')\")\n",
    "df2.dropDuplicates(['name', 'height']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `drop(col) `\n",
    "Recordad que drop elimina la columna pero devuelve un nuevo DataFrame, no modifica el que tenemos (son inmutables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminamos la columna age:\n",
      "[Row(name='Alice'), Row(name='Bob'), Row(name='Juan'), Row(name='Pepe'), Row(name='Panchito')]\n",
      "[Row(name='Alice'), Row(name='Bob'), Row(name='Juan'), Row(name='Pepe'), Row(name='Panchito')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Eliminamos la columna age:\")\n",
    "print(df.drop('age').collect())\n",
    "print(df.drop(df.age).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `limit(num)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit(1):[Row(name='Alice', age=1)]\n",
      "Limit(0):[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Limit(1):\" + str( df.limit(1).collect() ))\n",
    "print(\"Limit(0):\" + str( df.limit(0).collect() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `withColumn(colName, col) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Añadimos una columna denominada age2 obtenida a partir de la columna age y sumando un 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1, age2=3),\n",
       " Row(name='Bob', age=4, age2=6),\n",
       " Row(name='Juan', age=10, age2=12),\n",
       " Row(name='Pepe', age=25, age2=27),\n",
       " Row(name='Panchito', age=15, age2=17)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Añadimos una columna denominada age2 obtenida a partir de la columna age y sumando un 2:\")\n",
    "df.withColumn('age2', df.age + 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `withColumnRenamed(existing, new) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renombramos la columna age a age2 y obtenemos un nuevo DataFrame con el cambio:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age2=1),\n",
       " Row(name='Bob', age2=4),\n",
       " Row(name='Juan', age2=10),\n",
       " Row(name='Pepe', age2=25),\n",
       " Row(name='Panchito', age2=15)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Renombramos la columna age a age2 y obtenemos un nuevo DataFrame con el cambio:\")\n",
    "df.withColumnRenamed('age', 'age2').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones: Operaciones con columnas\n",
    "\n",
    "\n",
    "Operación | Descripción\n",
    "------------- | -------------\n",
    "*alias(*\\**alias)* | Devuelve la columna con un nuevo nombre\n",
    "*between(lowerBound, upperBound)* | True si el valor está entre los dos valores\n",
    "*isNull() / isNotNull()* | True si el valor es nulo y viceversa\n",
    "*when(condition, value) / otherwise(value)* | Evalúa una lista de condiciones y en base a estas devuelve un valor u otro\n",
    "*Startswith(other), substring(startPos, len), like(otheR)* | Funciones para operar con strings\n",
    "*isin(*\\**cols)* | True si el valor está en la lista de argumentos\n",
    "*explode(col)* | Devuelve una nueva fila para cada elemento en el array\n",
    "*lit(value)* | Crea una columna con el valor literal\n",
    "*length(col)* | Longitud de la columna\n",
    "\n",
    "Estas operaciones por si mismas solo devuelve una columna (expresión SQL). Para poder aplicalas al DataFrame se deben utilizar en conjunto con una transformación tipo `select`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `alias(*alias)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna cambio de nombre: \n",
      "Column<'name AS hola'>\n",
      "\n",
      "DataFrame con columna nombre renombrada a hola: \n",
      "+--------+\n",
      "|    hola|\n",
      "+--------+\n",
      "|   Alice|\n",
      "|     Bob|\n",
      "|    Juan|\n",
      "|    Pepe|\n",
      "|Panchito|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Columna cambio de nombre: \")\n",
    "print(df.name.alias(\"hola\"))\n",
    "\n",
    "print(\"\\nDataFrame con columna nombre renombrada a hola: \")\n",
    "df.select(df.name.alias(\"hola\")).show() # Solo aparece la columna seleccionada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `between(lowerBound, upperBound) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna con condición between: \n",
      "Column<'((age >= 18) AND (age <= 65))'>\n",
      "\n",
      "DataFrame con nombre y columna con True o False según se cumple la condición\n",
      "+--------+---------------------------+\n",
      "|    name|((age >= 2) AND (age <= 4))|\n",
      "+--------+---------------------------+\n",
      "|   Alice|                      false|\n",
      "|     Bob|                       true|\n",
      "|    Juan|                      false|\n",
      "|    Pepe|                      false|\n",
      "|Panchito|                      false|\n",
      "+--------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Columna con condición between: \")\n",
    "print(df.age.between(18, 65))\n",
    "print(\"\\nDataFrame con nombre y columna con True o False según se cumple la condición\")\n",
    "df.select(df.name, df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `isNull() / isNotNull() `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expresión Columna con isNull()/isNotNull():\n",
      "Column<'(age IS NULL)'>\n",
      "Column<'(age IS NOT NULL)'>\n",
      "\n",
      "DataFrame con filas que tienen el nombre nulo\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Expresión Columna con isNull()/isNotNull():\")\n",
    "print(df.age.isNull())\n",
    "print(df.age.isNotNull())\n",
    "\n",
    "print(\"\\nDataFrame con filas que tienen el nombre nulo\")\n",
    "df.filter(df.name.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `when(condition, value) / otherwise(value) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condición when/otherwise:\n",
      "Column<'CASE WHEN (age > 18) THEN adulto ELSE peque END'>\n",
      "\n",
      "Ejemplos when/otherwise\n",
      "+--------+-------------------------------------+\n",
      "|    name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "+--------+-------------------------------------+\n",
      "|   Alice|                                    0|\n",
      "|     Bob|                                    1|\n",
      "|    Juan|                                    1|\n",
      "|    Pepe|                                    1|\n",
      "|Panchito|                                    1|\n",
      "+--------+-------------------------------------+\n",
      "\n",
      "+--------+------------------------------------------------------------+\n",
      "|    name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      "+--------+------------------------------------------------------------+\n",
      "|   Alice|                                                          -1|\n",
      "|     Bob|                                                           0|\n",
      "|    Juan|                                                           1|\n",
      "|    Pepe|                                                           1|\n",
      "|Panchito|                                                           1|\n",
      "+--------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Condición when/otherwise:\")\n",
    "print(F.when(df.age > 18, \"adulto\").otherwise(\"peque\"))\n",
    "\n",
    "print(\"\\nEjemplos when/otherwise\")\n",
    "df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
    "df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Startswith(other), substring(startPos, len), like(otheR) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna like: \n",
      "Column<'name LIKE M*'>\n",
      "\n",
      "DataFrame con primeras tres letras de los nombres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Inicial='Ali'),\n",
       " Row(Inicial='Bob'),\n",
       " Row(Inicial='Jua'),\n",
       " Row(Inicial='Pep'),\n",
       " Row(Inicial='Pan')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columna like: \")\n",
    "print(df.name.like(\"M*\"))\n",
    "\n",
    "print(\"\\nDataFrame con primeras tres letras de los nombres\")\n",
    "df.select(df.name.substr(1, 3).alias(\"Inicial\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `isin(*cols) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna con expresión isin()\n",
      "Column<'(name IN (Mikel, Pepe))'>\n",
      "\n",
      "Filtrar filas con nombres Bob y Mikel\n",
      "[Row(name='Bob', age=4)]\n",
      "Filtrar filas con edades 1, 2 o 3\n",
      "[Row(name='Alice', age=1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Columna con expresión isin()\")\n",
    "print(df.name.isin(\"Mikel\", \"Pepe\"))\n",
    "\n",
    "# Ojo! Estamos usando la operación para filtrar las filas, para ello podemos usar df[condición]!\n",
    "print(\"\\nFiltrar filas con nombres Bob y Mikel\")\n",
    "print(df[df.name.isin(\"Bob\", \"Mikel\")].collect())\n",
    "print(\"Filtrar filas con edades 1, 2 o 3\")\n",
    "print(df[df.age.isin([1, 2, 3])].collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `explode(col) `\n",
    "Explode puede utilizarse para simular el flatMap de los RDDs, ya que crea tantas filas como valores tenga un array o map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame original\n",
      "[Row(a=1, intlist=[1, 2, 3], mapfield={'a': 'b'})]\n",
      "\n",
      "DataFrame con filas expandidas según el array de enteros\n",
      "+-----+\n",
      "|anInt|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n",
      "DataFrame con filas expandidas según el map de caracteres\n",
      "+---+---------+--------+---+-----+\n",
      "|  a|  intlist|mapfield|key|value|\n",
      "+---+---------+--------+---+-----+\n",
      "|  1|[1, 2, 3]|{a -> b}|  a|    b|\n",
      "+---+---------+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "print(\"DataFrame original\")\n",
    "print(eDF.collect())\n",
    "\n",
    "print(\"\\nDataFrame con filas expandidas según el array de enteros\")\n",
    "eDF.select(F.explode(eDF.intlist).alias(\"anInt\")).show()\n",
    "\n",
    "print(\"DataFrame con filas expandidas según el map de caracteres\")\n",
    "eDF.select('*', F.explode(eDF.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lit(value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna de unos: \n",
      "Column<'1'>\n",
      "\n",
      "DataFrame con una nueva columna con unos:\n",
      "+--------+---+---+\n",
      "|    name|age|  1|\n",
      "+--------+---+---+\n",
      "|   Alice|  1|  1|\n",
      "|     Bob|  4|  1|\n",
      "|    Juan| 10|  1|\n",
      "|    Pepe| 25|  1|\n",
      "|Panchito| 15|  1|\n",
      "+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Columna de unos: \")\n",
    "print(F.lit(1))\n",
    "\n",
    "print(\"\\nDataFrame con una nueva columna con unos:\")\n",
    "df.select(\"*\", F.lit(1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `length(col) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna para obtener longitud del texto: \n",
      "Column<'length(name)'>\n",
      "\n",
      "DataFrame con la longitud de cada nombre\n",
      "+---+\n",
      "|len|\n",
      "+---+\n",
      "|  5|\n",
      "|  3|\n",
      "|  4|\n",
      "|  4|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Columna para obtener longitud del texto: \" )\n",
    "print(F.length(df.name))\n",
    "\n",
    "print(\"\\nDataFrame con la longitud de cada nombre\")\n",
    "df.select(F.length(df.name).alias('len')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones con pseudo-conjuntos\n",
    "Aunque no los hemos visto en teoría, podemos hacer operaciones entre pseudo-conjuntos al igual que con los RDDs\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*distinct()* | Devuelve el DataFrame sin elementos repetidos – ¡Cuidado! Requiere shuffle (enviar datos por red)\n",
    "*union(rdd)* | Devuelve la unión de los elementos en los dos DataFrame  (se mantienen los duplicados)\n",
    "*intersect(rdd)* | Devuelve la instersección de los elementos en los dos DataFrame (elimina los duplicados) – ¡Cuidado! Requiere shuffle (datos por red)\n",
    "*subtract(rdd)* | Devuelve los elementos presentes en el primer DataFrame y no en el segundo – ¡Cuidado! También requiere de shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct: [Row(value='agua'), Row(value='cerveza'), Row(value='vino')]\n",
      "union: [Row(value='agua'), Row(value='vino'), Row(value='cerveza'), Row(value='agua'), Row(value='agua'), Row(value='vino'), Row(value='cerveza'), Row(value='cerveza'), Row(value='agua'), Row(value='agua'), Row(value='vino'), Row(value='coca-cola'), Row(value='naranjada')]\n",
      "intersect: [Row(value='agua'), Row(value='cerveza'), Row(value='vino')]\n",
      "substract: []\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([\"agua\", \"vino\", \"cerveza\", \"agua\", \"agua\", \"vino\"], \"string\")\n",
    "df2 = spark.createDataFrame([\"cerveza\", \"cerveza\", \"agua\", \"agua\", \"vino\", \"coca-cola\", \"naranjada\"], \"string\")\n",
    "\n",
    "print(\"distinct: \" + str(df1.distinct().collect()))\n",
    "print(\"union: \" + str(df1.union(df2).collect()))\n",
    "print(\"intersect: \" + str(df1.intersect(df2).collect()))\n",
    "print(\"substract: \" + str(df1.subtract(df2).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones sobre RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones básicas\n",
    "\n",
    "\n",
    "Acción | Descripción\n",
    "------------- | -------------\n",
    "*show(n=20, truncate=True)* | Imprime las primeras n filas del DataFrame. Truncate indica si se quiere truncar los strings demasiado largos\n",
    "*count()* | Devuelve el número de filas en el DataFrame\n",
    "*collect()* | Devuelve todas las filas del DataFrame como una lista de Rows **Cuidado: Debe de caber en memoria**\n",
    "*first()* | Devuelve la primera fila del DataFrame\n",
    "*take(n)* | Devuelve las primeras n filas del DataFrame como lista de Rows\n",
    "*toPandas()* | Devuelve el contenido del DataFrame como un pandas.DataFrame **Cuidado: Debe de caber en memoria**\n",
    "*columns* | Devuelve todos los nombres de columnas como una lista\n",
    "*describe(*\\**cols)* | Calcula estadísticas para las columnas numéricas  (count, mean, stddev, min, y max)\n",
    "*explain(extended=False)* | Imprime los planes físicos y lógicos para debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame show(): \n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|   Alice|  1|\n",
      "|     Bob|  4|\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n",
      "Dos primeras filas con take(2): [Row(name='Alice', age=1), Row(name='Bob', age=4)]\n",
      "\n",
      "Primera fila con first(): Row(name='Alice', age=1)\n",
      "\n",
      "DataFrame completo con collect(): [Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n",
      "\n",
      "Número de filas en el DataFrame con count(): 5\n",
      "\n",
      "DataFrame como panda: \n",
      "       name  age\n",
      "0     Alice    1\n",
      "1       Bob    4\n",
      "2      Juan   10\n",
      "3      Pepe   25\n",
      "4  Panchito   15\n",
      "\n",
      "Columnas en el DataFrame con columns: ['name', 'age']\n",
      "\n",
      "Planes físicos y lógicos con explain:\n",
      "== Parsed Logical Plan ==\n",
      "Project [age#107L]\n",
      "+- Filter (age#107L > cast(10 as bigint))\n",
      "   +- LogicalRDD [name#106, age#107L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "age: bigint\n",
      "Project [age#107L]\n",
      "+- Filter (age#107L > cast(10 as bigint))\n",
      "   +- LogicalRDD [name#106, age#107L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [age#107L]\n",
      "+- Filter (isnotnull(age#107L) AND (age#107L > 10))\n",
      "   +- LogicalRDD [name#106, age#107L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [age#107L]\n",
      "+- *(1) Filter (isnotnull(age#107L) AND (age#107L > 10))\n",
      "   +- *(1) Scan ExistingRDD[name#106,age#107L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame show(): \")\n",
    "df.show()\n",
    "\n",
    "print(\"Dos primeras filas con take(2): \" + str(df.take(2)))\n",
    "\n",
    "print(\"\\nPrimera fila con first(): \" + str(df.first()))\n",
    "\n",
    "print(\"\\nDataFrame completo con collect(): \" + str( df.collect() ))\n",
    "\n",
    "print(\"\\nNúmero de filas en el DataFrame con count(): \" + str( df.count() ) )\n",
    "\n",
    "print(\"\\nDataFrame como panda: \")\n",
    "print(df.toPandas())\n",
    "\n",
    "print(\"\\nColumnas en el DataFrame con columns: \"+ str( df.columns ))\n",
    "\n",
    "print(\"\\nPlanes físicos y lógicos con explain:\")\n",
    "df.filter(df.age > 10).select(df.age).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones para realizar agregaciones\n",
    "\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*agg(*\\**exprs)* | Realiza agregaciones sobre el DataFrame completo sin agrupar (equivalente a df.groupBy.agg()) – Agregaciones disponibles: avg, max, min, sum, count. exprs puede ser un diccionario clave (nombre columna) – valor (función de agregación) o una lista de expresiones de agregación de Columna\n",
    "*groupBy(*\\**cols)* | Agrupa el DataFrame usando las columnas especificadas para poder aplicar agregaciones sobre los grupos – GroupedData\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `agg(*exprs)`\n",
    "Agrega columnas numéricas en todo el DataFrame, sin grupos.\n",
    "\n",
    "Agregaciones disponibles: avg, max, min, sum, count\n",
    "\n",
    "\\**exprs* puede ser\n",
    "\n",
    "* Un diccionario clave (nombre columna) – valor (función de agregación)\n",
    "+ Una lista de expresiones de agregación de Columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando diccionario clave-valor: \n",
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      25|\n",
      "+--------+\n",
      "\n",
      "Usando expresión de agregación en la columna:\n",
      "+--------+\n",
      "|min(age)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Usando diccionario clave-valor: \")\n",
    "df.agg({\"age\": \"max\"}).show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "print(\"Usando expresión de agregación en la columna:\")\n",
    "df.agg(F.min(df.age)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupBy(*cols)`\n",
    "Agrupa filas del DataFrame en base a las columnas especificadas\n",
    "\n",
    "Se crea un DataFrame GroupedData, posteriormente se trata de aplicar agregaciones por grupos (ver siguiente sección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media usando groupBy().avg() - Se agrupa todo el DataFrame = usar agg() directamente\n",
      "[Row(avg(age)=11.0)]\n",
      "\n",
      "Agrupamos por nombre y para cada nombre calculamos la edad media:\n",
      "[Row(name='Alice', avg(age)=1.0), Row(name='Bob', avg(age)=4.0), Row(name='Juan', avg(age)=10.0), Row(name='Panchito', avg(age)=15.0), Row(name='Pepe', avg(age)=25.0)]\n",
      "[Row(name='Alice', avg(age)=1.0), Row(name='Bob', avg(age)=4.0), Row(name='Juan', avg(age)=10.0), Row(name='Panchito', avg(age)=15.0), Row(name='Pepe', avg(age)=25.0)]\n",
      "\n",
      "Agrupamos por nombre y edad y contamos cuántos filas hay en cada grupo:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1, count=1),\n",
       " Row(name='Bob', age=4, count=1),\n",
       " Row(name='Juan', age=10, count=1),\n",
       " Row(name='Panchito', age=15, count=1),\n",
       " Row(name='Pepe', age=25, count=1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Media usando groupBy().avg() - Se agrupa todo el DataFrame = usar agg() directamente\")\n",
    "print(df.groupBy().avg().collect())\n",
    "\n",
    "print(\"\\nAgrupamos por nombre y para cada nombre calculamos la edad media:\")\n",
    "print(sorted(df.groupBy('name').agg({'age': 'mean'}).collect()))\n",
    "print(sorted(df.groupBy(df.name).avg().collect()))\n",
    "\n",
    "print(\"\\nAgrupamos por nombre y edad y contamos cuántos filas hay en cada grupo:\")\n",
    "sorted(df.groupBy(['name', df.age]).count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre GroupedData (groupBy)\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*avg(*\\**cols) / mean(*\\**cols)* | Calcula la media de los valores para cada grupo en cada columna numérica\n",
    "*count()* | Cuenta el número de registros (filas) en cada grupo\n",
    "*max(**\\**cols)* | Calcula el máximo valor para cada grupo en cada columna numérica\n",
    "*min(*\\**cols)* | Calcula el mínimo valor para cada grupo en cada columna numérica\n",
    "*sum(*\\**cols)* | Calcula la suma de los valores para cada grupo en cada columna numérica\n",
    "*pivot(pivot_col, values)* | Pivota sobre una columna del DataFrame para realizar después la agregación especificada. Values especifica los valores que aparecerán en las columnas. Si no se especifica lo calcula Spark (menos eficiente)\n",
    "*agg(*\\**exprs)* | Realiza agregaciones sobre cada grupo del DataFrame. Agregaciones disponibles: avg, max, min, sum, count. exprs puede ser un diccionario clave (nombre columna) – valor (función de agregación) o una lista de expresiones de agregación de Columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `avg(*cols) / mean(*cols), count(), max(*cols), min(*cols), sum(*cols)`\n",
    "Podemos aplicar directamente cualquiera de estos métodos sobre GroupedData para obtener el resultado de la función correspondiente por grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame ejemplo\n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "|  Bob|  5|    90|\n",
      "|  Bob| 10|   100|\n",
      "+-----+---+------+\n",
      "\n",
      "Edad media global con avg: [Row(avg(age)=7.0)]\n",
      "Edad y altura medias globales con avg: [Row(avg(age)=7.0, avg(height)=86.0)]\n",
      "Edad media global con mean[Row(avg(age)=7.0)]\n",
      "Edad y altura medias globales con mean[Row(avg(age)=7.0, avg(height)=86.0)]\n",
      "Media de edad y altura agrupoda por nombre: \n",
      "+-----+-----------------+-----------+\n",
      "| name|         avg(age)|avg(height)|\n",
      "+-----+-----------------+-----------+\n",
      "|Alice|6.666666666666667|       80.0|\n",
      "|  Bob|              7.5|       95.0|\n",
      "+-----+-----------------+-----------+\n",
      "\n",
      "Conteo de personas por edad:\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "|  5|    3|\n",
      "| 10|    2|\n",
      "+---+-----+\n",
      "\n",
      "Edad máxima global[Row(max(age)=10)]\n",
      "Edad y altura máximas globales[Row(max(age)=10, max(height)=100)]\n",
      "Edad mínima global[Row(min(age)=5)]\n",
      "Edad y altura minima globales[Row(min(age)=5, min(height)=80)]\n",
      "Suma global de edades[Row(sum(age)=35)]\n",
      "Suma global de edades y altura[Row(sum(age)=35, sum(height)=430)]\n"
     ]
    }
   ],
   "source": [
    "df2 = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80), \\\n",
    "    Row(name='Bob', age=5, height=90), \\\n",
    "    Row(name='Bob', age=10, height=100)]).toDF()\n",
    "\n",
    "print(\"DataFrame ejemplo\")\n",
    "df2.show()\n",
    "\n",
    "print(\"Edad media global con avg: \" + str( df2.groupBy().avg('age').collect() ))\n",
    "print(\"Edad y altura medias globales con avg: \" + str( df2.groupBy().avg('age', 'height').collect() ))\n",
    "\n",
    "print(\"Edad media global con mean\" + str(df2.groupBy().mean('age').collect() ))\n",
    "print(\"Edad y altura medias globales con mean\" + str( df2.groupBy().mean('age', 'height').collect() ))\n",
    "\n",
    "\n",
    "print(\"Media de edad y altura agrupoda por nombre: \")\n",
    "df2.groupBy('name').avg('age', 'height').show()\n",
    "\n",
    "print(\"Conteo de personas por edad:\")\n",
    "df2.groupBy(df2.age).count().show()\n",
    "\n",
    "print(\"Edad máxima global\" + str( df2.groupBy().max('age').collect() ) )\n",
    "print(\"Edad y altura máximas globales\" + str( df2.groupBy().max('age', 'height').collect() ))\n",
    "\n",
    "\n",
    "print(\"Edad mínima global\" + str( df2.groupBy().min('age').collect() ))\n",
    "print(\"Edad y altura minima globales\" + str( df2.groupBy().min('age', 'height').collect() ))\n",
    "\n",
    "print(\"Suma global de edades\" + str( df2.groupBy().sum('age').collect() ))\n",
    "print(\"Suma global de edades y altura\" + str( df2.groupBy().sum('age', 'height').collect() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edad media agrupado por nombre con avg: \n",
      "+-----+-----------------+\n",
      "| name|         avg(age)|\n",
      "+-----+-----------------+\n",
      "|Alice|6.666666666666667|\n",
      "|  Bob|              7.5|\n",
      "+-----+-----------------+\n",
      "\n",
      "Altura media agrupado por edad con avg: \n",
      "+---+-----------------+\n",
      "|age|      avg(height)|\n",
      "+---+-----------------+\n",
      "|  5|83.33333333333333|\n",
      "| 10|             90.0|\n",
      "+---+-----------------+\n",
      "\n",
      "Conteo de personas por altura:\n",
      "+------+-----+\n",
      "|height|count|\n",
      "+------+-----+\n",
      "|    80|    3|\n",
      "|   100|    1|\n",
      "|    90|    1|\n",
      "+------+-----+\n",
      "\n",
      "Edad máxima por nombre\n",
      "+-----+--------+\n",
      "| name|max(age)|\n",
      "+-----+--------+\n",
      "|Alice|      10|\n",
      "|  Bob|      10|\n",
      "+-----+--------+\n",
      "\n",
      "Edad media por nombre y altura\n",
      "+-----+------+-----------------+\n",
      "| name|height|         avg(age)|\n",
      "+-----+------+-----------------+\n",
      "|Alice|    80|6.666666666666667|\n",
      "|  Bob|   100|             10.0|\n",
      "|  Bob|    90|              5.0|\n",
      "+-----+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Edad media agrupado por nombre con avg: \" )\n",
    "df2.groupBy('name').avg('age').show()\n",
    "\n",
    "print(\"Altura media agrupado por edad con avg: \" )\n",
    "df2.groupBy('age').avg('height').show()\n",
    "\n",
    "print(\"Conteo de personas por altura:\")\n",
    "df2.groupBy(df2.height).count().show()\n",
    "\n",
    "print(\"Edad máxima por nombre\" )\n",
    "df2.groupBy('name').max('age').show()\n",
    "\n",
    "print(\"Edad media por nombre y altura\" )\n",
    "df2.groupBy('name', 'height').avg('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `agg(*exprs)`\n",
    "Agrega columnas numéricas por grupos\n",
    "\n",
    "Agregaciones disponibles: avg, max, min, sum, count\n",
    "\n",
    "\\**exprs* puede ser\n",
    "\n",
    "* Un diccionario clave (nombre columna) – valor (función de agregación)\n",
    "+ Una lista de expresiones de agregación de Columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de personas con el mismo nombre:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Conteo de personas con el mismo nombre:\")\n",
    "df2.agg({\"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edad mínima para cada nombre:\n",
      "+-----+--------+\n",
      "| name|min(age)|\n",
      "+-----+--------+\n",
      "|Alice|       5|\n",
      "|  Bob|       5|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Edad mínima para cada nombre:\")\n",
    "df2.groupBy(df2.name).agg(F.min(df2.age)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de edad y notas de todos los alumnos\n",
      "+--------+----------+\n",
      "|avg(age)|avg(grade)|\n",
      "+--------+----------+\n",
      "|     2.5|       7.5|\n",
      "+--------+----------+\n",
      "\n",
      "Media de edad y notas de los alumnos con el mismo nombre\n",
      "+-----+--------+----------+\n",
      "| name|avg(age)|avg(grade)|\n",
      "+-----+--------+----------+\n",
      "|Alice|     2.0|       7.5|\n",
      "|  Bob|     3.0|       7.5|\n",
      "+-----+--------+----------+\n",
      "\n",
      "Número de alumnos con el mismo nombre\n",
      "+-----+--------+\n",
      "| name|count(1)|\n",
      "+-----+--------+\n",
      "|Alice|       2|\n",
      "|  Bob|       2|\n",
      "+-----+--------+\n",
      "\n",
      "+-----+-----+\n",
      "| name|count|\n",
      "+-----+-----+\n",
      "|Alice|    2|\n",
      "|  Bob|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alice',1,6, 'Mate'), ('Bob',2,8, 'Mate'), ('Alice',3,9, 'Lengua'), ('Bob',4,7, 'Lengua')]\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'grade', 'subject'])\n",
    "\n",
    "print(\"Media de edad y notas de todos los alumnos\")\n",
    "df.groupBy().avg().show()\n",
    "\n",
    "print(\"Media de edad y notas de los alumnos con el mismo nombre\")\n",
    "df.groupBy('name').avg('age', 'grade').show()\n",
    "\n",
    "print(\"Número de alumnos con el mismo nombre\")\n",
    "df1 = df.groupBy(df.name)\n",
    "df1.agg({\"*\": \"count\"}).show() \n",
    "df.groupBy(df.name).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pivot(pivot_col, values)`\n",
    "\n",
    "Pivota sobre una columna del DataFrame para realizar después la agregación especificada\n",
    "\n",
    "*values* especifica los valores que aparecerán en las columnas\n",
    "* Si no se especifica lo calcula Spark (menos eficiente)\n",
    "* Similar a pivot table de pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notas medias por alumno y asignatura (modo eficiente especificando asignaturas): \n",
      "+-----+----+------+\n",
      "| name|Mate|Lengua|\n",
      "+-----+----+------+\n",
      "|  Bob| 8.0|   7.0|\n",
      "|Alice| 6.0|   9.0|\n",
      "+-----+----+------+\n",
      "\n",
      "Notas medias por alumno y asignatura (modo no eficiente, dejando a Spark que busque los valores sobre los que pivotar): \n",
      "+-----+------+----+\n",
      "| name|Lengua|Mate|\n",
      "+-----+------+----+\n",
      "|  Bob|   7.0| 8.0|\n",
      "|Alice|   9.0| 6.0|\n",
      "+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Notas medias por alumno y asignatura (modo eficiente especificando asignaturas): \")\n",
    "df.groupBy(\"name\").pivot(\"subject\", [\"Mate\", \"Lengua\"]).avg(\"grade\").show()\n",
    "\n",
    "print(\"Notas medias por alumno y asignatura (modo no eficiente, dejando a Spark que busque los valores sobre los que pivotar): \")\n",
    "df.groupBy(\"name\").pivot(\"subject\").avg(\"grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspectos avanzados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario (User Defined Functions, UDF)\n",
    "\n",
    "Podemos crear una función para trabajar sobre las columnas de los DataFrames mediante funciones definidas por el usuario. Sin embargo, debemos limitarnos a lo estríctamente necesario al ser mucho menos eficientes que las nativas de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud con función definida por el usuario (UDF):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 233:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|slen|\n",
      "+----+\n",
      "|   5|\n",
      "|   3|\n",
      "|   5|\n",
      "|   3|\n",
      "+----+\n",
      "\n",
      "Longitud con versión de SparkSQL, mucho más eficiente:\n",
      "+------+\n",
      "|length|\n",
      "+------+\n",
      "|     5|\n",
      "|     3|\n",
      "|     5|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Longitud con función definida por el usuario (UDF):\")\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "df.select(slen(df.name).alias('slen')).show()\n",
    "\n",
    "print(\"Longitud con versión de SparkSQL, mucho más eficiente:\")\n",
    "from pyspark.sql.functions import length\n",
    "df.select(length(df.name).alias('length')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL en Spark\n",
    "Podemos trabajar directamente con SQL en Spark si registramos el DataFrame como tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+\n",
      "| name|age|grade|subject|\n",
      "+-----+---+-----+-------+\n",
      "|Alice|  1|    6|   Mate|\n",
      "|  Bob|  2|    8|   Mate|\n",
      "|Alice|  3|    9| Lengua|\n",
      "|  Bob|  4|    7| Lengua|\n",
      "+-----+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+\n",
      "| name|age|grade|subject|\n",
      "+-----+---+-----+-------+\n",
      "|Alice|  3|    9| Lengua|\n",
      "|  Bob|  4|    7| Lengua|\n",
      "+-----+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM people WHERE age > 2 AND subject != 'Mate'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINs con SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones Join tipo SQL\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*join(other, on, how)*  | Joins entre dos DataFrames\n",
    "\n",
    "* `other`: segunda tabla para el join\n",
    "* `on`: nombre de la columna por la que se realiza la unión. Puede ser una lista o una expresión join (Column)\n",
    "* `how`: tipo de unión entre inner, `outer, left_outer, right_outer, left_semi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:\n",
      "+-----+----+\n",
      "|  age|name|\n",
      "+-----+----+\n",
      "|Alice|   1|\n",
      "|  Bob|   2|\n",
      "+-----+----+\n",
      "\n",
      "df2:\n",
      "+------+----+\n",
      "|height|name|\n",
      "+------+----+\n",
      "| Chris|  80|\n",
      "|   Bob|  85|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [Row(name=u'Alice', age=1), Row(name=u'Bob', age=2)]\n",
    "data2 = [Row(name=u'Chris', height=80), Row(name=u'Bob', height=85)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['age', 'name'])\n",
    "df2 = spark.createDataFrame(data2, ['height', 'name'])\n",
    "\n",
    "print(\"df:\")\n",
    "df.show()\n",
    "\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `inner join`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join por name\n",
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "+----+---+------+\n",
      "\n",
      "Join por name + select df.name y height\n",
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Join por name\")\n",
    "df.join(df2, 'name').show()\n",
    "\n",
    "print(\"Join por name + select df.name y height\")\n",
    "df.join(df2, 'name').select(df.name, df2.height).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fullOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full outer join por name\n",
      "+----+-----+------+\n",
      "|name|  age|height|\n",
      "+----+-----+------+\n",
      "|   1|Alice|  null|\n",
      "|   2|  Bob|  null|\n",
      "|  80| null| Chris|\n",
      "|  85| null|   Bob|\n",
      "+----+-----+------+\n",
      "\n",
      "Full outer join por name + select df.name y height\n",
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "|   1|  null|\n",
      "|   2|  null|\n",
      "|  80| Chris|\n",
      "|  85|   Bob|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Full outer join por name\")\n",
    "df.join(df2, 'name', 'outer').show()\n",
    "\n",
    "print(\"Full outer join por name + select df.name y height\")\n",
    "df.join(df2, 'name', 'outer').select('name', 'height').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `leftOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left outer join por name\n",
      "+----+-----+------+\n",
      "|name|  age|height|\n",
      "+----+-----+------+\n",
      "|   1|Alice|  null|\n",
      "|   2|  Bob|  null|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Left outer join por name\")\n",
    "df.join(df2, 'name', 'left_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rightOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left outer join por name\n",
      "+----+----+------+\n",
      "|name| age|height|\n",
      "+----+----+------+\n",
      "|  80|null| Chris|\n",
      "|  85|null|   Bob|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Left outer join por name\")\n",
    "df.join(df2, 'name', 'right_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:\n",
      "+-----+------+-----+-------+\n",
      "| name|course|grade|subject|\n",
      "+-----+------+-----+-------+\n",
      "|Alice|     1|    6|   Mate|\n",
      "|  Bob|     2|    8|   Mate|\n",
      "|Alice|     3|    9| Lengua|\n",
      "|  Bob|     4|    7| Lengua|\n",
      "+-----+------+-----+-------+\n",
      "\n",
      "df2:\n",
      "+-------+---+------+\n",
      "|   name|age|height|\n",
      "+-------+---+------+\n",
      "|  Alice|  5|    80|\n",
      "|  Alice|  5|    80|\n",
      "|  Alice| 10|    80|\n",
      "|    Bob|  5|    90|\n",
      "|    Bob| 10|   100|\n",
      "|   Juan|  5|    90|\n",
      "|Jaimito|  6|   120|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alice',1,6, 'Mate'), \\\n",
    "        ('Bob',2,8, 'Mate'), \\\n",
    "        ('Alice',3,9, 'Lengua'), \\\n",
    "        ('Bob',4,7, 'Lengua')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'course', 'grade', 'subject'])\n",
    "df2 = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80), \\\n",
    "    Row(name='Bob', age=5, height=90), \\\n",
    "    Row(name='Bob', age=10, height=100), \\\n",
    "    Row(name='Juan', age=5, height=90), \\\n",
    "    Row(name='Jaimito', age=6, height=120)]).toDF()\n",
    "\n",
    "print(\"df:\")\n",
    "df.show()\n",
    "\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join usando name\n",
      "+-----+------+-----+-------+---+------+\n",
      "| name|course|grade|subject|age|height|\n",
      "+-----+------+-----+-------+---+------+\n",
      "|Alice|     1|    6|   Mate|  5|    80|\n",
      "|Alice|     1|    6|   Mate|  5|    80|\n",
      "|Alice|     1|    6|   Mate| 10|    80|\n",
      "|Alice|     3|    9| Lengua|  5|    80|\n",
      "|Alice|     3|    9| Lengua|  5|    80|\n",
      "|Alice|     3|    9| Lengua| 10|    80|\n",
      "|  Bob|     2|    8|   Mate|  5|    90|\n",
      "|  Bob|     2|    8|   Mate| 10|   100|\n",
      "|  Bob|     4|    7| Lengua|  5|    90|\n",
      "|  Bob|     4|    7| Lengua| 10|   100|\n",
      "+-----+------+-----+-------+---+------+\n",
      "\n",
      "Outer join usando name + select name y height\n",
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|  Alice|    80|\n",
      "|  Alice|    80|\n",
      "|  Alice|    80|\n",
      "|  Alice|    80|\n",
      "|  Alice|    80|\n",
      "|  Alice|    80|\n",
      "|    Bob|    90|\n",
      "|    Bob|   100|\n",
      "|    Bob|    90|\n",
      "|    Bob|   100|\n",
      "|Jaimito|   120|\n",
      "|   Juan|    90|\n",
      "+-------+------+\n",
      "\n",
      "Outer join usando df.name == df2.name + select df.name y height\n",
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|    80|\n",
      "|Alice|    80|\n",
      "|Alice|    80|\n",
      "|Alice|    80|\n",
      "|Alice|    80|\n",
      "|Alice|    80|\n",
      "|  Bob|    90|\n",
      "|  Bob|   100|\n",
      "|  Bob|    90|\n",
      "|  Bob|   100|\n",
      "| null|   120|\n",
      "| null|    90|\n",
      "+-----+------+\n",
      "\n",
      "Outer join usando condición con name y age = grade (a pesar de no tener sentido) + select name y age\n",
      "+-----+----+\n",
      "| name| age|\n",
      "+-----+----+\n",
      "| null|   5|\n",
      "| null|   5|\n",
      "|Alice|null|\n",
      "|Alice|null|\n",
      "| null|  10|\n",
      "| null|   5|\n",
      "|  Bob|null|\n",
      "|  Bob|null|\n",
      "| null|  10|\n",
      "| null|   6|\n",
      "| null|   5|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Inner join usando name\")\n",
    "df.join(df2, 'name').show()\n",
    "\n",
    "print(\"Outer join usando name + select name y height\")\n",
    "df.join(df2, 'name', 'outer').select('name', 'height').show()\n",
    "\n",
    "print(\"Outer join usando df.name == df2.name + select df.name y height\")\n",
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).show()\n",
    "\n",
    "print(\"Outer join usando condición con name y age = grade (a pesar de no tener sentido) + select name y age\")\n",
    "cond = [df.name == df2.name, df2.age == df.grade]\n",
    "df.join(df2, cond, 'outer').select(df.name, df2.age).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caché de DataFrames\n",
    "Si se va a reusar un DataFrame es conveniente cachearlo, así no se recalcula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 300:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabeza aparece 230 veces\n",
      "Lanza aparece 90 veces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "quijoteDF = spark.read.text(\"./datos/pg2000.txt\")\n",
    "palabrasQuijoteDF = quijoteDF.select(explode(split('value', ' ')).alias('word')).cache()\n",
    "print(\"Cabeza aparece \" + str( palabrasQuijoteDF.where(palabrasQuijoteDF.word.like(\"%cabeza%\")).count() ) + \" veces\")\n",
    "print(\"Lanza aparece \" + str( palabrasQuijoteDF.where(palabrasQuijoteDF.word.like(\"%lanza%\")).count() ) + \" veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y escritura de ficheros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura\n",
    "Leer ficheros de texto, JSON o CVS es muy sencillo.\n",
    "\n",
    "Hay dos formas de hacerlo:\n",
    "* Usando `spark.read.tipo_fichero(fichero)`\n",
    "* Usando `spark.read.format(tipo_fichero).options(opciones).load(fichero)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en DataFrame a partir de datos/pg2000.txt: 37861\n",
      "Esquema: \n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n",
      "Elementos en DataFrame a partir de datos/json.json: 5\n",
      "Esquema: \n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "None\n",
      "Elementos en DataFrame a partir de datos/personas.csv: 100\n",
      "Esquema: \n",
      "root\n",
      " |-- Persona: string (nullable = true)\n",
      " |-- Tel: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dfText = spark.read.text(\"datos/pg2000.txt\")\n",
    "dfJSON = spark.read.json(\"datos/json.json\") # infiere el esquema\n",
    "dfCSV = spark.read.csv(\"datos/personas.csv\", inferSchema=True, header=True)\n",
    "\n",
    "print(\"Elementos en DataFrame a partir de datos/pg2000.txt: \" + str(dfText.count()) + \"\\nEsquema: \")\n",
    "print (dfText.printSchema())\n",
    "\n",
    "print(\"Elementos en DataFrame a partir de datos/json.json: \" + str(dfJSON.count()) + \"\\nEsquema: \")\n",
    "print (dfJSON.printSchema())\n",
    "\n",
    "print(\"Elementos en DataFrame a partir de datos/personas.csv: \" + str(dfCSV.count()) + \"\\nEsquema: \")\n",
    "print (dfCSV.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en DataFrame a partir de datos/pg2000.txt: 37861\n",
      "Esquema: \n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n",
      "Elementos en DataFrame a partir de datos/json.json: 5\n",
      "Esquema: \n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "None\n",
      "Elementos en DataFrame a partir de datos/personas.csv: 100\n",
      "Esquema: \n",
      "root\n",
      " |-- Persona: string (nullable = true)\n",
      " |-- Tel: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dfText = spark.read.format(\"text\").load(\"datos/pg2000.txt\")\n",
    "dfJSON = spark.read.format(\"json\").load(\"datos/json.json\") # infiere el esquema\n",
    "dfCSV = spark.read.format(\"csv\").options(inferSchema=True, header=True).load(\"datos/personas.csv\")\n",
    "\n",
    "print (\"Elementos en DataFrame a partir de datos/pg2000.txt: \" + str(dfText.count()) + \"\\nEsquema: \")\n",
    "print (dfText.printSchema())\n",
    "\n",
    "print (\"Elementos en DataFrame a partir de datos/json.json: \" + str(dfJSON.count()) + \"\\nEsquema: \")\n",
    "print (dfJSON.printSchema())\n",
    "\n",
    "print (\"Elementos en DataFrame a partir de datos/personas.csv: \" + str(dfCSV.count()) + \"\\nEsquema: \")\n",
    "print (dfCSV.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura\n",
    "Escribir ficheros de texto, JSON o CVS es igual de fácil.\n",
    "\n",
    "**Nota: El fichero de salida se toma como directorio**\n",
    "\n",
    "Hay dos formas de hacerlo:\n",
    "* Usando `DataFrame.write.tipo_fichero(fichero)`\n",
    "* Usando `DataFrame.write.format(tipo_fichero).options(opciones).load(fichero)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ver datos escritos en datos/salidaTXT1.txt\n",
      "Ver datos escritos en datos/salidaJSON1.json\n",
      "Ver datos escritos en datos/salidaCSV1.csv\n"
     ]
    }
   ],
   "source": [
    "dfText.write.text(\"datos/salidaTXT1.txt\")\n",
    "dfJSON.write.json(\"datos/salidaJSON1.json\")\n",
    "dfCSV.write.csv(\"datos/salidaCSV1.csv\", header=True)\n",
    "\n",
    "print(\"Ver datos escritos en datos/salidaTXT1.txt\")\n",
    "print(\"Ver datos escritos en datos/salidaJSON1.json\")\n",
    "print(\"Ver datos escritos en datos/salidaCSV1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/alumno/Descargas/P2/datos/salidaTXT2.txt already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11962/1072392809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datos/salidaTXT2.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfJSON\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datos/salidaJSON2.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdfCSV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datos/salidaCSV2.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ver datos escritos en datos/salidaTXT2.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/alumno/Descargas/P2/datos/salidaTXT2.txt already exists."
     ]
    }
   ],
   "source": [
    "## ¡¡CUIDADO FALLA SI EXISTE!!\n",
    "dfText.write.format('text').save(\"datos/salidaTXT2.txt\")\n",
    "dfJSON.write.format('json').save(\"datos/salidaJSON2.json\")\n",
    "dfCSV.write.format('csv').save(\"datos/salidaCSV2.csv\", header=True)\n",
    "\n",
    "print(\"Ver datos escritos en datos/salidaTXT2.txt\")\n",
    "print(\"Ver datos escritos en datos/salidaJSON2.json\")\n",
    "print(\"Ver datos escritos en datos/salidaCSV2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más opciones en: <http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un ejemplo completo: WordCount simple con DataFrames\n",
    "\n",
    "*value* es el nombre de la columna por defecto al leer texto\n",
    "\n",
    "1. Aplicamos un select con explode que equivale a un flatMap\n",
    "1. Dentro del select dividimos la línea por espacios con split\n",
    " * A la columna resultante la denominamos Word con alias\n",
    " * Filtramos palabras vacías\n",
    "2. Agrupamos por palabra y contamos cuántas veces se repite cada una\n",
    "3. Ordenamos de manera descendente\n",
    "4. Mostramos el DataFrame obtenido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 345:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| que|19429|\n",
      "|  de|17988|\n",
      "|   y|15894|\n",
      "|  la|10200|\n",
      "|   a| 9575|\n",
      "|  el| 7957|\n",
      "|  en| 7898|\n",
      "|  no| 5611|\n",
      "|  se| 4690|\n",
      "| los| 4680|\n",
      "| con| 4047|\n",
      "| por| 3758|\n",
      "| las| 3423|\n",
      "|  lo| 3387|\n",
      "|  le| 3382|\n",
      "|  su| 3319|\n",
      "| don| 2533|\n",
      "| del| 2464|\n",
      "|  me| 2344|\n",
      "|como| 2226|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dfText = spark.read.format(\"text\").load(\"datos/pg2000.txt\")\n",
    "dfText.select(explode(split(dfText[\"value\"], \" \")).alias(\"word\")) \\\n",
    "    .filter(\"word != ''\") \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .sort(desc(\"count\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfText.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vemos lo que hay en el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|\n",
      "|                                                                           |\n",
      "|This eBook is for the use of anyone anywhere at no cost and with           |\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or       |\n",
      "|re-use it under the terms of the Project Gutenberg License included        |\n",
      "|with this eBook or online at www.gutenberg.net                             |\n",
      "|                                                                           |\n",
      "|                                                                           |\n",
      "|Title: Don Quijote                                                         |\n",
      "|                                                                           |\n",
      "+---------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select(\"*\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividimos las líneas en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|split(value,  , -1)                                                                     |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|[The, Project, Gutenberg, EBook, of, Don, Quijote,, by, Miguel, de, Cervantes, Saavedra]|\n",
      "|[]                                                                                      |\n",
      "|[This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with]         |\n",
      "|[almost, no, restrictions, whatsoever., , You, may, copy, it,, give, it, away, or]      |\n",
      "|[re-use, it, under, the, terms, of, the, Project, Gutenberg, License, included]         |\n",
      "|[with, this, eBook, or, online, at, www.gutenberg.net]                                  |\n",
      "|[]                                                                                      |\n",
      "|[]                                                                                      |\n",
      "|[Title:, Don, Quijote]                                                                  |\n",
      "|[]                                                                                      |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select(split(\"value\", \" \")) \\\n",
    "      .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usamos explode para crear una Row con cada elemento del array en la columna actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      col|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    EBook|\n",
      "|       of|\n",
      "|      Don|\n",
      "| Quijote,|\n",
      "|       by|\n",
      "|   Miguel|\n",
      "|       de|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select( \\\n",
    "       explode(split(\"value\", \" \"))) \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renombramos la nueva columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    EBook|\n",
      "|       of|\n",
      "|      Don|\n",
      "| Quijote,|\n",
      "|       by|\n",
      "|   Miguel|\n",
      "|       de|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select( \\\n",
    "       explode(split(\"value\", \" \")) \\\n",
    "      .alias(\"word\")) \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtramos palabras vacías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    EBook|\n",
      "|       of|\n",
      "|      Don|\n",
      "| Quijote,|\n",
      "|       by|\n",
      "|   Miguel|\n",
      "|       de|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select( \\\n",
    "       explode(split(\"value\", \" \")) \\\n",
    "      .alias(\"word\")) \\\n",
    "      .filter(\"word != ''\") \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupamos por palabra y contamos el número de apariciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|      online|    4|\n",
      "|      saben,|    3|\n",
      "|      Platón|    1|\n",
      "|     tempora|    1|\n",
      "|Comentarios,|    1|\n",
      "|       fuere|   64|\n",
      "|       ense-|    1|\n",
      "|          tú|  185|\n",
      "|      gloria|   35|\n",
      "|      formó,|    1|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select( \\\n",
    "       explode(split(\"value\", \" \")) \\\n",
    "      .alias(\"word\")) \\\n",
    "      .filter(\"word != ''\") \\\n",
    "      .groupBy(\"word\").count() \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordenamos de manera descendente por la cuenta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| que|19429|\n",
      "|  de|17988|\n",
      "|   y|15894|\n",
      "|  la|10200|\n",
      "|   a| 9575|\n",
      "|  el| 7957|\n",
      "|  en| 7898|\n",
      "|  no| 5611|\n",
      "|  se| 4690|\n",
      "| los| 4680|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select( \\\n",
    "       explode(split(\"value\", \" \")) \\\n",
    "      .alias(\"word\")) \\\n",
    "      .filter(\"word != ''\") \\\n",
    "      .groupBy(\"word\").count() \\\n",
    "      .sort(desc(\"count\")) \\\n",
    "      .show(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
