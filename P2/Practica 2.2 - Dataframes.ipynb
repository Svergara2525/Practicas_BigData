{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIof-Wrv3Sl7"
   },
   "source": [
    "# Grado en ciencia de datos - Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8KIxxud3Sl9"
   },
   "source": [
    "# Práctica 2 - Parte II - Ejercicios SparkSQL\n",
    "\n",
    "En este segundo notebook de la práctica 2 se deberán realizar varios ejercicios haciendo uso de los DataFrames de Spark. \n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sicIOgop3Sl9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 13:25:54 WARN Utils: Your hostname, MacBook-Pro-de-Sergio.local resolves to a loopback address: 127.0.0.1; using 192.168.1.13 instead (on interface en0)\n",
      "25/12/07 13:25:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 13:25:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49796)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/tmp/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYggNDY63Sl-"
   },
   "source": [
    "Otros imports necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GJZH-Cge3Sl-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from test_helper import Test\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPheHNl3Sl_"
   },
   "source": [
    "## Ejercicio 1. Contar palabras\n",
    "\n",
    "Crear una función MapReduce en Spark llamada `cuentaPalabras(filePath)` que cuente cuántas veces aparece cada palabra en un documento o conjunto de documentos de entrada. Utiliza DataFrames.\n",
    "\n",
    "**Entrada:** Documento o documentos\n",
    "\n",
    "**Salida:** (Palabra, Número de apariciones)\n",
    "\n",
    "Pasos a seguir:\n",
    "1. Leer el fichero. Cada línea es un elemento del DataFrame (columna value).\n",
    "2. Dividir las líneas en palabras.\n",
    "3. Filtrar palabras vacías.\n",
    "4. Contar las occurrencias de cada palabra.\n",
    "5. Devolver al driver las 10 palabras más repetidas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "id": "ENn4ozV43Sl_",
    "nbgrader": {
     "checksum": "add06ef0d1bf0a07e0398c6180721317",
     "grade": false,
     "grade_id": "cell-4a0285858b38f01b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cuentaPalabras(filePath):\n",
    "    dfText = spark.read.text(filePath)\n",
    "    \n",
    "    return dfText.select(F.explode(F.split(dfText[\"value\"], \" \")).alias(\"word\")) \\\n",
    "                 .filter(\"word != ''\") \\\n",
    "                 .groupBy(\"word\").count() \\\n",
    "                 .sort(F.desc(\"count\")) \\\n",
    "                 .take(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2RSs5WJ3Sl_"
   },
   "source": [
    "El programa debe ser capaz de pasar los siguientes tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mIMVAMS03Sl_",
    "nbgrader": {
     "checksum": "8328443ca6c8d6970ec12ba3eb1b9561",
     "grade": true,
     "grade_id": "cell-c52393fbe687e2af",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(word='que', count=19429), Row(word='de', count=17988), Row(word='y', count=15894), Row(word='la', count=10200), Row(word='a', count=9575), Row(word='el', count=7957), Row(word='en', count=7898), Row(word='no', count=5611), Row(word='se', count=4690), Row(word='los', count=4680)]\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "top10Quijote = cuentaPalabras(\"./datos/pg2000.txt\")\n",
    "print(top10Quijote)\n",
    "Test.assertEquals(top10Quijote, [(u'que', 19429), (u'de', 17988), (u'y', 15894), (u'la', 10200), \n",
    "                          (u'a', 9575), (u'el', 7957), (u'en', 7898), (u'no', 5611), \n",
    "                          (u'se', 4690), (u'los', 4680)],\n",
    "                  'Resultado incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0lvvqhf3SmB"
   },
   "source": [
    "### Parte 2 - Mejorando la cuenta de palabras\n",
    "Utiliza la función `eliminarPuntuacion(text)` para contar todas las palabras igual independientemente de las mayúsculas, los signos de puntuación etc. Utilizar DataFrames.\n",
    "\n",
    "Pasos a seguir:\n",
    "1. Leer el fichero. Cada línea es un elemento del DataFrame (columna value).\n",
    "2. Eliminar los signos de puntuación.\n",
    "2. Dividir las líneas en palabras.\n",
    "3. Filtrar palabras vacías.\n",
    "4. Contar las occurrencias de cada palabra.\n",
    "5. Devolver al driver las 10 palabras más repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UrIlsn-I3SmC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|frase                                    |\n",
      "+-----------------------------------------+\n",
      "|Hola!, Qué tal?                          |\n",
      "| Sin barras_bajas!                       |\n",
      "| *      Elimina puntación y espacios  * ,|\n",
      "+-----------------------------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|frase                       |\n",
      "+----------------------------+\n",
      "|hola qué tal                |\n",
      "|sin barrasbajas             |\n",
      "|elimina puntación y espacios|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def eliminaSignosPuntuacion(column):\n",
    "    \"\"\"Elimina los signos de puntuación, pasa las palabras a minúsculas y elimina los espacios de más antes y después\n",
    "    Args:\n",
    "        column (Column): Una columna con una frase\n",
    "\n",
    "    Returns:\n",
    "        Column: Una columna llamada frase con la frase original limpia\n",
    "    \"\"\"\n",
    "    return lower(trim(regexp_replace(column, r'[^0-9a-zA-ZñÑáéíóúÁÉÍÓÚ ]+', ''))).alias('frase')\n",
    "\n",
    "fraseDF = spark.createDataFrame([(u'Hola!, Qué tal?',),\n",
    "                                         (u' Sin barras_bajas!',),\n",
    "                                         (u' *      Elimina puntación y espacios  * ,',)], ['frase'])\n",
    "fraseDF.show(truncate=False)\n",
    "(fraseDF\n",
    "       .select(eliminaSignosPuntuacion(col('frase')))\n",
    "       .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "id": "P92o7OmN3SmE",
    "nbgrader": {
     "checksum": "89dc897cf2b91d966b129747aecc6722",
     "grade": false,
     "grade_id": "cell-15bae227a52520b2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cuentaPalabras(filePath):\n",
    "    dfText = spark.read.text(filePath)\n",
    "    \n",
    "    return dfText.select(eliminaSignosPuntuacion(col(\"value\"))) \\\n",
    "                 .select(F.explode(F.split(col(\"frase\"), \" \")).alias(\"word\")) \\\n",
    "                 .filter(\"word != ''\") \\\n",
    "                 .groupBy(\"word\").count() \\\n",
    "                 .sort(F.desc(\"count\")) \\\n",
    "                 .take(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dcr87UW33SmE",
    "nbgrader": {
     "checksum": "0ef5c0c81c0dfdc7be992ae5803436a7",
     "grade": true,
     "grade_id": "cell-8193ba3a20ea3dfa",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "top10Quijote = cuentaPalabras(\"./datos/pg2000.txt\")\n",
    "Test.assertEquals(top10Quijote, [(u'que', 20626), (u'de', 18217), (u'y', 18188), (u'la', 10363), (u'a', 9880),\n",
    "                                 (u'en', 8241), (u'el', 8210), (u'no', 6345), (u'los', 4748), (u'se', 4690)],\n",
    "                  'Resultado incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm_CVfmC3SmE"
   },
   "source": [
    "## Ejercicio 2. Histograma de repeticiones\n",
    "\n",
    "Obtener un histograma del número de repeticiones de las palabras, es decir, cuántas palabras se repiten X veces:\n",
    "* 1 vez – 3 palabras\n",
    "* 2 veces – 10 palabras\n",
    "* 3 veces – 20 palabras\n",
    "* ...\n",
    "\n",
    "Para ello crea una función MapReduce en Spark llamada `histogramaRepeticiones(filePath)`. Esta función NO debe hacerr uso de la función `cuentaPalabras(filePath)` del programa anterior aunque compartirá parte de su código. Todo debe realizarse mediante DafaFrames salvo el `collect()` final que devolverá una lista. La lista debe de estar ordenada por el número de veces. Continúa utilizando la función `eliminarPuntuacion(text)` para contar igual todas las palabras.\n",
    "\n",
    "**Entrada:** Documento o documentos\n",
    "\n",
    "**Salida:** (X veces, Número de palabras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "id": "uVkJSq1T3SmE",
    "nbgrader": {
     "checksum": "cd9b7f68b7e37c951349d36af1fc2bfe",
     "grade": false,
     "grade_id": "cell-1feeff04bed1d241",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def histogramaRepeticiones(filePath):\n",
    "    dfText = spark.read.text(filePath)\n",
    "    \n",
    "    return dfText.select(eliminaSignosPuntuacion(col(\"value\"))) \\\n",
    "                 .select(F.explode(F.split(col(\"frase\"), \" \")).alias(\"word\")) \\\n",
    "                 .filter(\"word != ''\") \\\n",
    "                 .groupBy(\"word\").count()\\\n",
    "                 .groupBy(\"count\").count()\\\n",
    "                 .sort(F.asc(\"count\")) \\\n",
    "                 .take(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EGA-dns3SmF"
   },
   "source": [
    "El programa debe ser capaz de pasar los siguientes tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hxJ7JmIK3SmF",
    "nbgrader": {
     "checksum": "fec40960043b05a191f91369e3fa54ff",
     "grade": true,
     "grade_id": "cell-e139d274db6accdd",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "histQuijote = histogramaRepeticiones(\"./datos/pg2000.txt\")\n",
    "Test.assertEquals(histQuijote[:10], [(1, 11583), (2, 3664), (3, 1860), (4, 1147), (5, 780), (6, 552), \n",
    "                                     (7, 422), (8, 334), (9, 261), (10, 227)],\n",
    "                  'Resultado incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rDUtHH13SmF"
   },
   "source": [
    "Podemos realizar un gráficos a partir de los datos con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "I0rGZlM83SmF"
   },
   "outputs": [],
   "source": [
    "### Rellenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZs9AKj23SmF"
   },
   "source": [
    "## Ejercicio 3. Análisis sobre los tweets de las elecciones de EEUU de 2012\n",
    "\n",
    "En lo que resta de práctica vamos a hacer uso del dataset disponible en la siguiente URL <https://datahub.io/dataset/twitter-2012-presidential-election>.\n",
    "\n",
    "Este dataset contiene millones de tweets recogidos durante la campaña electoral de 2012 cuando se enfrentaban Romney (republicano) frente a Obama (demócrata). Se ha llegado a decir que una de las cosas que hizo ganar a Obama fue el uso del Big Data y el análisis de las redes sociales. De manera similar, nosotros vamos a trabajar con estos tweets para ver de lo que es capaz SparkSQL, desde la carga de datos a su análisis.\n",
    "\n",
    "En nuestro caso, debido al tamaño del dataset utilizaremos solo la primera parte (400MB comprimidas y 3GB descomprimida). \n",
    "\n",
    "En primer lugar, leemos el dataset (en formato JSON) y lo cacheamos, para no tener que leerlo continuamente de disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WjW_O7E93SmF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 13:26:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "datosJSON = spark.read.json('datos/tweets2012/cache-0.json').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4rjT1ol3SmG"
   },
   "source": [
    "#### ¿Cuántos tweets tenemos disponibles para analizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "id": "RNDgGRB43SmG",
    "nbgrader": {
     "checksum": "48025f521b6817e17c82828f53365f00",
     "grade": false,
     "grade_id": "cell-7bda98f62295e8e3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_7 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 ERROR Executor: Exception in task 7.0 in stage 19.0 (TID 62)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "25/12/07 13:26:03 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 19.0 (TID 62),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "25/12/07 13:26:03 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@44e45edb rejected from java.util.concurrent.ThreadPoolExecutor@64cff38f[Shutting down, pool size = 12, active threads = 12, queued tasks = 0, completed tasks = 55]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "25/12/07 13:26:03 ERROR TaskSetManager: Task 7 in stage 19.0 failed 1 times; aborting job\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_11 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_9 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_0 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_3 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_10 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_6 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Putting block rdd_43_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_8 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_2 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_4 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_5 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN BlockManager: Block rdd_43_1 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 5.0 in stage 19.0 (TID 60) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 6.0 in stage 19.0 (TID 61) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 9.0 in stage 19.0 (TID 64) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 3.0 in stage 19.0 (TID 58) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 55) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 10.0 in stage 19.0 (TID 65) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 11.0 in stage 19.0 (TID 66) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 8.0 in stage 19.0 (TID 63) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 4.0 in stage 19.0 (TID 59) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 1.0 in stage 19.0 (TID 56) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 13:26:03 WARN TaskSetManager: Lost task 2.0 in stage 19.0 (TID 57) (192.168.1.13 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 19.0 failed 1 times, most recent failure: Lost task 7.0 in stage 19.0 (TID 62) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:285)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2398/0x0000000601bf2e98.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2776/0x0000000601cdf7b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/77/6t1fp7hd3bsbwjkb0pg8hbv40000gn/T/ipykernel_2972/1653509487.py\", line 1, in <module>\n",
      "    nTweets = datosJSON.count()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1234, in count\n",
      "    return int(self._jdf.count())\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nTweets \u001b[38;5;241m=\u001b[39m \u001b[43mdatosJSON\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m3\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/ipykernel/zmqshell.py:587\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    581\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    582\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    584\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    588\u001b[0m }\n\u001b[1;32m    590\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py311ml/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "nTweets = datosJSON.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IjYFiNEJ3SmG",
    "nbgrader": {
     "checksum": "399faba1a732fd9f3e1b029f1d6aa729",
     "grade": true,
     "grade_id": "cell-0cd1af8b7ef2d6da",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(nTweets, 1000000, \"El cálculo del número de tweets es incorrecto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl4T4y1c3SmG"
   },
   "source": [
    "### Imprime el esquema del DataFrame y la primera fila para ver qué tipo de datos tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "UfQ7UHuW3SmG",
    "nbgrader": {
     "checksum": "c6fe100fc381dd5019b1051ae23bc127",
     "grade": false,
     "grade_id": "cell-20559fe0d5a2bb13",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "datosJSON.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcH8L1V-3SmG"
   },
   "outputs": [],
   "source": [
    "datosJSON.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i-GRv8o3SmH"
   },
   "source": [
    "### Filtrado de campos\n",
    "Como puedes observar, tenemos una gran cantidad de campos correspondientes a cada tweet. Sin embargo, no todos son interesantes o no nos van a ser últiles para trabajar en esta práctica. Por ello, será más sencillo si solo nos quedamos con los campos que van a ser útiles.\n",
    "\n",
    "### Crea un nuevo DataFrame, denominado `df`, que contenga solo los datos correspondientes a las siguientes columnas\n",
    "* id\n",
    "* user.name\n",
    "* user.followers_count\n",
    "* text\n",
    "* retweet_count\n",
    "* place.country\n",
    "* entities.user_mentions\n",
    "* entities.hashtags\n",
    "* created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "uN9K5fGy3SmH",
    "nbgrader": {
     "checksum": "d49d5907f1542abb7baf1f2976bff5aa",
     "grade": false,
     "grade_id": "cell-40d1ed0a397995ab",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "df = datosJSON.select('id', 'user.name', 'user.followers_count', 'text', 'retweet_count', 'place.country', 'entities.user_mentions', 'entities.hashtags', 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LxMORWVB3SmH",
    "nbgrader": {
     "checksum": "aaad139b5b97a4fb4b7c9d2777885a32",
     "grade": true,
     "grade_id": "cell-38543999e3631bd3",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(df.columns, ['id', 'name', 'followers_count', 'text', 'retweet_count',\n",
    "                               'country', 'user_mentions', 'hashtags', 'created_at'],\n",
    "                 'Columnas seleccionadas incorrectas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvwsQHBx3SmH"
   },
   "source": [
    "## Ejercicios básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ9Xj34w3SmH"
   },
   "source": [
    "### 1. Obtener la media de la longitud de los tweets por usuario ordenado de mayor a menor. ¿Quién es el usuario con mayor media de longitud? - considera solo aquellos que hayan publicado más de 100 tweets\n",
    "\n",
    "Pasos a seguir:\n",
    "1. Utiliza un select para obtener junto con cada nombre de usuario la longitud del tweet (usar función `length`)\n",
    "2. Agrupar por nombre y obtener la media de la longitud y el conteo\n",
    "3. Filtrar por conteo\n",
    "5. Ordenar por media de la longitud\n",
    "6. Obtener el nombre del primer usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "2J_jEBNE3SmH",
    "nbgrader": {
     "checksum": "3b6d7bd3253775040a4c67c4d3afa55a",
     "grade": false,
     "grade_id": "cell-db8b90a43bd8e3d9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Devolver solo el nombre para pasar el test\n",
    "usuario = df.select(df.name, F.length(col(\"text\")).alias(\"lenTweet\")) \\\n",
    "            .groupBy(\"name\").agg(F.count(\"lenTweet\"), F.mean(\"lenTweet\")) \\\n",
    "            .filter(\"count(lenTweet) > 100\") \\\n",
    "            .sort(F.desc(\"avg(lenTweet)\")) \\\n",
    "            .first()[0]#Devuelvo solo el nombre para pasar el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W0CKbTpb3SmI",
    "nbgrader": {
     "checksum": "58e90af9c17704d60a71d7d8453e1d1f",
     "grade": true,
     "grade_id": "cell-d0b369592efcde98",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(usuario, u'Cypress Gang', 'Usuario incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvF-W3ef3SmI"
   },
   "source": [
    "### 2. Obtener el número de tweets por país para los tweets que tienen el país establecido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "DOV75nwx3SmI",
    "nbgrader": {
     "checksum": "c201db101ba9846973e13d65d92b223e",
     "grade": false,
     "grade_id": "cell-245f9e65dfcb4d82",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Si en alguna ejecución no pasa el test porque cambia las posiciones de Spain e Indonesia está bien\n",
    "tweetsPorPaisDF = df.filter(df.country.isNotNull()) \\\n",
    "                    .groupBy(\"country\").count() \\\n",
    "                    .sort(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tweetsPorPaisDF.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IcCRov2m3SmI",
    "nbgrader": {
     "checksum": "a01bfbc9242b5e809450c6848d750ae7",
     "grade": true,
     "grade_id": "cell-06acf085d11e0003",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(tweetsPorPaisDF.take(10), [(u'United States', 5240), (u'Brasil', 1212), (u'United Kingdom', 386), (u'Germany', 223), \n",
    "                                             (u'Indonesia', 188), (u'Spain', 188), (u'Mexico', 174), (u'Italy', 150), (u'Canada', 121), \n",
    "                                             (u'France', 94)], 'Tweets por país incorrectos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M44fhZf3SmI"
   },
   "source": [
    "### 3. Obtener el listado de los usuarios más mencionados\n",
    "A tener en cuenta:\n",
    "* Las menciones a otros usuarios aparecen en el campo `user_mentions` que es un array. La forma de desempaquetar el array es mediante la función explode. Una vez aplicada, se puede realizar otro select para quedarnos solo con el nombre del usuario `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "RbLbR4R63SmI",
    "nbgrader": {
     "checksum": "91ecc847ad73cd2a67863d06dc99de32",
     "grade": false,
     "grade_id": "cell-ac3a8856c562ac53",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "mencionUsuariosDF = df.select(F.explode(\"user_mentions\").alias(\"fila\")) \\\n",
    "                      .select(F.col('fila.name')) \\\n",
    "                      .groupBy(\"name\").count() \\\n",
    "                      .sort(F.desc(\"count\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4Yyo9hDX3SmJ",
    "nbgrader": {
     "checksum": "7a9c512a017e587119f40a2d07a6fd75",
     "grade": true,
     "grade_id": "cell-96d4236a7bfacaf4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(mencionUsuariosDF.take(10), [(u'Barack Obama', 13895), (u'Nicki Minaj', 9744), (u'YouTube', 5127), \n",
    "                                               (u'Mitt Romney', 4069), (u'Bill Maher', 3908), (u'ShareThis', 3876), \n",
    "                                               (u'2Chainz (Tity Boi)', 3862), (u'Most Funniest Man', 3059), \n",
    "                                               (u'PublicPolicyPolling', 2776), (u'Top Tweets \\u2655', 2097)],\n",
    "                  'Tweets por país incorrectos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVA8WgBJ3SmJ"
   },
   "source": [
    "## Hashtags más populares\n",
    "\n",
    "Vamos a realizar un análisis de los hashtags más populares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyq2LNdB3SmJ"
   },
   "source": [
    "#### Imprime el esquema del DataFrame actual (`df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "XXeH_Sez3SmJ",
    "nbgrader": {
     "checksum": "b73601364137df8b78bea74e3d27e6a8",
     "grade": false,
     "grade_id": "cell-3b0c9a43446607f7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaXIGUyU3SmJ"
   },
   "source": [
    "#### Vamos a estudiar cuáles han sido los hashtags más populares\n",
    "Para ello, vamos a seguir los siguientes pasos:\n",
    "1. Los hashtags vienen dados por un array. La función `explode` nos permite desempaquetar el array y convertir cada el elemento del array en una fila. Denominaremos a esta columna 'tags'\n",
    "2. Puedes observar (printSchema) como cada elemento de la columna tags está formado por el índice y el texto. Solo nos interesa el texto y además vamos a convertirlo a minúsculas (función `lower`). La columna resultante se llamará 'tag'.\n",
    "3. Ya tenemos un DataFrame con todas las tags que han aparecido en los tweets que tenemos disponibles. Solo nos queda contar cuántas veces aparece cada tag (piensa como combinar `groupBy` con `count`).\n",
    "4. Por último, nos gustaría obtener el DataFrame ordenado de mayor a menor número de veces que ha sido usado un tag. \n",
    "5. Cachea el DataFrame obtenido ya que lo usaremos un par de veces y nos evitaremos volver a calcularlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "HSLuoz_Q3SmJ",
    "nbgrader": {
     "checksum": "868b988279511941ada9f1ff76762433",
     "grade": false,
     "grade_id": "cell-89801ec72f7f661b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "top10tags = df.select(F.explode(\"hashtags\").alias(\"tags\")) \\\n",
    "                      .select(F.lower(F.col('tags.text')).alias(\"text\")) \\\n",
    "                      .groupBy(\"text\").count() \\\n",
    "                      .sort(F.desc(\"count\"))\n",
    "\n",
    "top10tags.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "H_R9XxDg3SmK",
    "nbgrader": {
     "checksum": "e8593e87a75e65056a5f8ef7c8995a3b",
     "grade": true,
     "grade_id": "cell-d6861676e91c2978",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(top10tags.take(10), [(u'obama', 30643), (u'usa', 30405), \n",
    "                              (u'tcot', 19116), (u'p2', 8608), \n",
    "                              (u'romney', 6171), (u'news', 4785), \n",
    "                              (u'gop', 4600), (u'obama2012', 4179), \n",
    "                              (u'teaparty', 4057), (u'somalia', 3636)],\n",
    "                 'Tags obtenidas incorrectas')\n",
    "\n",
    "Test.assertEquals(top10tags.is_cached, True, 'DataFrame no cacheado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STAiUuaG3SmK"
   },
   "source": [
    "Los resultados obtenidos son curiosos, Obama es el hashtag más utilizado, seguido de cerca por USA, sin embargo, Romney, el candidato republicano aparece en muchos menos hashtags. ¿Tienen los republicanos menos presencia en twitter?\n",
    "\n",
    "Aunque la respuesta aparentemente podría ser sí, es interesante ver que aparecen otros hashtags como 'tcot', ¿a qué se refiere? El próximo artículo nos da la respuesta: <http://www.ibtimes.com/what-does-tcot-mean-about-tcot-hashtag-top-conservatives-use-twitter-1109812>\n",
    "\n",
    "En definitiva, los republicanos no se centraban tanto en el candidato, pero igualmente tenían presencia en twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuUfXH813SmK"
   },
   "source": [
    "#### Hashtags y menciones a usuarios\n",
    "Vamos a ver qué hashtags aparecen más veces junto con la mención a un mismo usuario. Esta vez nos olvidamos de pasarlo a mínusculas (no usar `lower`).\n",
    "\n",
    "Queremos obtener un DataFrame en el que tengamos, el nombre del usuario mencionado, el texto de la hashtag y el conteo de cuántas veces ha aparecido conjuntamente. El DataFrame debe estar ordenado por conteo.\n",
    "\n",
    "Recuerda que tanto los hashtags, como las menciones a usuarios están almacenadas en arrays que deben desempaquetarse. Para ello ten en cuenta que no puedes usar dos veces la función `explode` en el mismo select, y que por tanto debe de hacerse por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "NJMIoUQ83SmK",
    "nbgrader": {
     "checksum": "5cc57ef276d9b1f5fffea4eba5b633a0",
     "grade": false,
     "grade_id": "cell-97ebe56dd5917171",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "userHashtagDF = df.select(F.col(\"user_mentions\").alias(\"mentions\"), F.explode(\"hashtags\").alias(\"tags\")) \\\n",
    "                  .select(F.explode(\"mentions.name\").alias(\"user\"), F.col('tags.text').alias(\"text\")) \\\n",
    "                  .groupBy(\"user\", \"text\").count()\\\n",
    "                  .sort(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ePpw1m-a3SmK",
    "nbgrader": {
     "checksum": "d50222a042918a55de86945c22a1c337",
     "grade": true,
     "grade_id": "cell-d70197dc64e43f6a",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Si no pasa este test es posible que sea porque las siguientes líneas aparecen cambiadas:\n",
    "#|     RCTV_CONTIGO|    ChávezCADUCÓ| 1233|\n",
    "#|     RCTV_CONTIGO|   ChavezGranCob| 1175|\n",
    "# Si solo es eso, está OK\n",
    "Test.assertEquals(userHashtagDF.take(10), [(u'#TeamFollowBack', u'TeamFollowBack',  1487), (u'#TeamFollowBack', u'BillionDollarArt',  1458), \n",
    "                                           (u'#TeamFollowBack', u'USA',  1436), (u'#TeamFollowBack', u'NYC',  1372), \n",
    "                                           (u'RCTV_CONTIGO', u'Ch\\xe1vezCADUC\\xd3',  1231), (u'RCTV_CONTIGO', u'ChavezGranCob',  1173), \n",
    "                                           (u'CNN Breaking News', u'Obama',  834), (u'CNN Breaking News', u'Romney',  829), \n",
    "                                           (u'CNN Breaking News', u'CNNelections',  819), (u'Son of a Fratter', u'USA',  697)], \n",
    "                  'DataFrame de usuarios mencionados y hashtags incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD_uZLbP3SmL"
   },
   "source": [
    "#### Presencia en los tweets de cada uno de los candidatos\n",
    "En el apartado anterior sobre los hashtags solo nos hemos fijado en los hashtags, pero no hemos prestado atención al texto del mensaje. En el siguiente ejercicio vamos a tratar de ver en cuántos tweets estaba presente cada partido/candidato. Aunque debemos tener en cuenta que la mayor presencia no tiene porqué ser siempre buena (esto requeriría de un análisis mucho más profundo).\n",
    "\n",
    "Para simplificar la tarea asumiremos que las palabras relacionadas con Obama/demócratas son: obama y democrat; y las relacionadas Romney/republicanos son: romney, republican y tcot.\n",
    "\n",
    "Nuestro primer objetivo es crear un DataFrame en el que dispongamos de una columna que indique si menciona a Obama/demócratas  y otra si menciona a Romney/republicanos.\n",
    "\n",
    "Para llevar a cabo esta tarea, vamos a seguir los siguientes pasos.\n",
    "1. Crea un nuevo DataFrame a partir de `df`. A este DataFrame le vamos a añadir una nueva columna, llamada 'democrat', que nos indicará si en dicho tweet se ha mencionado alguna de las palabras correspondientes a los demócratas (obama, democrat). Si se ha mencionado alguna de ellas la columna tendrá el valor 'Democrat' y en otro caso el valor '-'. Para ello, haz uso de `withColumn` y las funciones `when/otherwise`. Considera siempre el texto en mínusculas (`lower`) para encontrar las coincidencias con `like`.\n",
    "2. Al DataFrame que hemos generado en el pimer punto, vamos a añadir otra columna igual pero para el caso de las menciones de los republicanos, la nueva columna se llamará 'Republican'.\n",
    "3. Obtener un DataFrame que nos indique qué porcentaje de los tweets mencionan algo que tienen que ver con los demócratas y en qué porcentaje no se les menciona. Columnas: democrat, porcentaje. El DataFrame se llamará democratDF. Para el cálculo del porcentaje, debemos utilizar el conteo total de tweets (nTweets).\n",
    "4. Obtener un DataFrame que nos indique qué porcentaje de los tweets mencionan algo que tienen que ver con los republicanos y en qué porcentaje no se les menciona. Columnas: republican, porcentaje. El DataFrame se llamará republicanDF.\n",
    "5. Por último, vamos a obtener un resumen más completo en el DataFrame partyDF. Para ello, vamos a añadir una nueva columna al DataFrame con democrat y republican que incluya la combinación de las dos columnas (`concat`). Posteriormente, obtendremos el porcentaje de tweets que corresponden a cada combinación. Así podemos analizar qué porcentaje de tweets mencionan solo a demócratas, solo a republicanos, a los dos a la vez o a ninguno.\n",
    "\n",
    "**Nota: Todos los DataFrames deben estar ordenados de mayor a menor porcentaje**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "yBzALKVP3SmL",
    "nbgrader": {
     "checksum": "b8097a9b9632d966107a670a23c05352",
     "grade": false,
     "grade_id": "cell-de6f3dc8ce29f9b1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "columnaDemocratDF = df.withColumn('democrat', F.when(F.lower(df.text).like('%obama%'), \"Democrat\") \\\n",
    "                      .otherwise(F.when(F.lower(df.text).like('%democrat%'), \"Democrat\") \\\n",
    "                      .otherwise(\"-\")))\n",
    "\n",
    "columnaRepublicanDemocratDF = columnaDemocratDF.withColumn('republican', F.when(F.lower(df.text).like('%romney%'), \"Republican\") \\\n",
    "                                               .otherwise(F.when(F.lower(df.text).like('%republican%'), \"Republican\") \\\n",
    "                                               .otherwise(F.when(F.lower(df.text).like('%tcot%'), \"Republican\") \\\n",
    "                                               .otherwise(\"-\"))))\n",
    "\n",
    "columnaRepublicanDemocratCombinadaDF = columnaRepublicanDemocratDF.select('republican', 'democrat')\n",
    "\n",
    "democratDF = columnaDemocratDF.groupBy('democrat') \\\n",
    "                              .count() \\\n",
    "                              .select('democrat', (100*F.col('count')/nTweets).alias('count')) \\\n",
    "                              .sort('count', ascending=False)\n",
    "\n",
    "republicanDF = columnaRepublicanDemocratDF.groupBy('republican') \\\n",
    "                                          .count() \\\n",
    "                                          .select('republican', (100*F.col('count')/nTweets).alias('count')) \\\n",
    "                                          .sort('count', ascending=False)\n",
    "\n",
    "dfParty = columnaRepublicanDemocratCombinadaDF.withColumn('concatenated', F.concat(*['republican', 'democrat'])) \\\n",
    "                                              .groupBy('concatenated') \\\n",
    "                                              .count() \\\n",
    "                                              .select('concatenated', (100*F.col('count')/nTweets).alias('count')) \\\n",
    "                                              .sort('count', ascending=False)\n",
    "\n",
    "# Mostrar dataframes\n",
    "democratDF.show()\n",
    "republicanDF.show()\n",
    "dfParty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wD2HQZso3SmL",
    "nbgrader": {
     "checksum": "625fce3367d261cf72aa7e925fb0ebd8",
     "grade": true,
     "grade_id": "cell-40ba55c2023cb063",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "round_fun = lambda res: list(map(lambda k_v: (k_v[0], np.round(k_v[1], 4)), res))\n",
    "Test.assertEquals(round_fun(democratDF.collect()), [(u'Democrat', 53.2483), (u'-', 46.7517)], \n",
    "                  'DataFrame de democratDF incorrecto')\n",
    "Test.assertEquals(round_fun(republicanDF.collect()), [(u'-', 83.3746), (u'Republican', 16.6254)], \n",
    "                  'DataFrame de republicanDF incorrecto')\n",
    "Test.assertEquals(round_fun(dfParty.collect()), [(u'--', 45.0071), (u'-Democrat', 38.3675), \n",
    "                                      (u'RepublicanDemocrat', 14.8808), (u'Republican-', 1.7446)], \n",
    "                  'DataFrame de dfParty incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOJ94GRe3SmL"
   },
   "source": [
    "De nuevo el resultado deja ver que los demócratas tienen más presencia. Quizás sea porque los republicanos únicamente se dedicaban a criticarlos o porque realmente los demócratas se movieron mucho más en las redes sociales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUwBemr33SmL"
   },
   "source": [
    "# Explorar más el dataset de tweets y nuevas conclusiones\n",
    "\n",
    "* Cuantificar el odio, contando el número de insultos (fuck, idiot, stupid, retard, moron, nutjob...) y calcular porcentajes para saber a qué partido iban más dirigidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX7y2dmr3SmL"
   },
   "source": [
    "Obtener un datafreme con el porcentaje de insultos a cada partido por cada país.\n",
    "\n",
    "1. La idea es combianar los ejercicios anteriores, seleccionar los tweets por país filtrando los nulos.\n",
    "2. Añadir una columna democrat: si en el Tweet aparece obama o democrat -> Democrat, sino -. El texto está en minúsculas.\n",
    "3. Añadir una columna republican: si en el Tweet aparece romney, republican o tcot -> Republican, sino -. El texto está en minúsculas.\n",
    "4. Añadir una columna insult: si en el Tweet aparece fuck, idiot, stupid, retard, moron o nutjob -> Yes, sino No. El texto está en minúsculas.\n",
    "5. Nos quedamos solo con los tweets en los que haya insultos, insult == YES.\n",
    "6. Concatenar las columnas democrat y republican, obteniendo las combinaciones. Agrupar por país y esta última columna creada, obteniendo el porcentaje de insultos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXEPpGKr3SmM"
   },
   "outputs": [],
   "source": [
    "#Nos quedamos con los datos que queremos, filtramos paises nulos \n",
    "df2 = df.select(F.col('text'), F.col('country')) \\\n",
    "                 .filter(F.col('country').isNotNull()) \n",
    "\n",
    "#Añadidos columna democrat.\n",
    "columnaDemocratDF = df2.withColumn('democrat', F.when(F.lower(F.col('text')).like('%obama%'), \"Democrat\") \\\n",
    "                        .otherwise(F.when(F.lower(F.col('text')).like('%democrat%'), \"Democrat\") \\\n",
    "                              .otherwise(\"-\")))\n",
    "\n",
    "#Añadidos columna republican.\n",
    "columnaRepublicanDemocratDF = columnaDemocratDF.withColumn('republican', F.when(F.lower(F.col('text')).like('%romney%'), \"Republican\") \\\n",
    "                                      .otherwise(F.when(F.lower(F.col('text')).like('%republican%'), \"Republican\") \\\n",
    "                                          .otherwise(F.when(F.lower(F.col('text')).like('%tcot%'), \"Republican\") \\\n",
    "                                              .otherwise(\"-\"))))\n",
    "\n",
    "#Añadidos columna insult.\n",
    "columnainsultDF = columnaRepublicanDemocratDF.withColumn('insult', F.when(F.lower(F.col('text')).like('%fuck%'), \"YES\") \\\n",
    "                                                 .otherwise(F.when(F.lower(F.col('text')).like('%idiot%'), \"YES\") \\\n",
    "                                                      .otherwise(F.when(F.lower(F.col('text')).like('%stupid%'), \"YES\") \\\n",
    "                                                          .otherwise(F.when(F.lower(F.col('text')).like('%retard%'), \"YES\") \\\n",
    "                                                              .otherwise(F.when(F.lower(F.col('text')).like('%moron%'), \"YES\") \\\n",
    "                                                                  .otherwise(F.when(F.lower(F.col('text')).like('%nutjob%'), \"YES\") \\\n",
    "                                                                      .otherwise(\"NO\")))))))\n",
    "\n",
    "#Seleccionamos las columnas que nos interesan e insult == YES.\n",
    "insultYesDF = columnainsultDF.select('country', 'republican', 'democrat', 'insult') \\\n",
    "                             .filter(F.col('insult') == 'YES')\n",
    "\n",
    "#Concatenamos las columnas de partido, agrupamos país y la concatenada ordenada por país el porcentaje de insultos.\n",
    "dfLast = insultYesDF.withColumn('concatenated', F.concat(*['republican', 'democrat'])) \\\n",
    "                    .groupBy('country', 'concatenated') \\\n",
    "                    .count() \\\n",
    "                    .select('country', 'concatenated', (100*F.col('count')/nTweets).alias('count')) \\\n",
    "                    .sort('country', ascending=False)\n",
    "\n",
    "#Mostramos el último dataframe.\n",
    "dfLast.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AeU1H_W3SmM"
   },
   "source": [
    "Escribir las conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYOYQMB03SmM"
   },
   "source": [
    "# Análisis final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dygDE4Jx3SmM"
   },
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyQnBZjt3SmM"
   },
   "source": [
    "Calcula el número de tweets por día y estudiar que día de la semana está la gente más activa en twiter. Quedarse solo con el día con más interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYRzKIBR3SmM"
   },
   "outputs": [],
   "source": [
    "weetsByDay = ###RELLENAR\n",
    "weetsByDay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwVWnuZF3SmM"
   },
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4QtJ0DX3SmN"
   },
   "source": [
    "Estudia el uso de twiter segun la hora del día. Mostrar solo los 10 primeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJGsAkeK3SmN"
   },
   "outputs": [],
   "source": [
    "histTweetsHour = ###RELLENAR\n",
    "histTweetsHour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hAHquvR3SmN"
   },
   "source": [
    "No se aprecia mucha información en la tabla mostrada, sin embargo sí nos ayuda a entender mejor el ejercicio anterior ya que vemos que efectivamente el día 10 de septiembre posee información de todas las horas (menos 15 y 16h) del día mientras que el resto de días no y esto lleva a que este día pareciera en el ejercicio anterior que había tenido más actividad.\n",
    "\n",
    "Para poder estudiar mejor el uso de twiter hora a hora vamos a filtrar por el día 10 y vamos a dibujar un histograma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhNw6K1-3SmN"
   },
   "outputs": [],
   "source": [
    "histTweetsHour = ###RELLENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCFvGMjh3SmN"
   },
   "outputs": [],
   "source": [
    "(x_values, y_values) = zip(*histTweetsHour)\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Histograma de tweets/hora')\n",
    "plt.xlabel('N. tweets')\n",
    "plt.ylabel('N. hora del día')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6Cv8-2z3SmN"
   },
   "source": [
    "Se observan dos picos y dos valles. Los picos coinciden con la noche y con el medio día (la hora de comer) aunque el segundo pico es mucho menor que el primero. Los valles se corresponden con las horas centrales de trabajo por la mañana o por la tarde (aunque no tenemos información entre las 14 y las 17h) y con las últimas horas de la madrugada. Se observa que a partir de las 2 a.m. los usuarios se van iendo a dormir y el número de tweets disminuye progresivamente hasta la hora de comer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiBDDCNl3SmN"
   },
   "source": [
    "### Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfrEnKI93SmO"
   },
   "source": [
    "Estudia el número de tweets relacionados con cada partido, que tengan la palabra (o partícula) \"win\" y la palabra o partícula \"lose\".\n",
    "\n",
    "En primer lugar concluimos que no hay muchos tweets que contengan las palabras \"win\" y \"lose\". Al margen de esto podemos ver que por lo general hay una mayor tendencia hacia considerar a un partido ganador que al otro perdedor, es decir, predominan los tweets de refuerzo positivo. Por otro lado destacar que hay mas tweets que consideran ganador al partido demócrata pero también es mayor el número de los que consideran que van a perder, por lo que es dificil extraer conclusiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9kRsTcU3SmO"
   },
   "outputs": [],
   "source": [
    "columnaDemocratDF = ###RELLENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOwWAzsr3SmO"
   },
   "outputs": [],
   "source": [
    "columnaRepublicanDemocratDF = ###RELLENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y6qhpNx3SmO"
   },
   "outputs": [],
   "source": [
    "columnastatusDF = ###RELLENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bxf8QhCq3SmO"
   },
   "outputs": [],
   "source": [
    "dfLast = ###RELLENAR\n",
    "dfLast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DU7BSf4R3SmO"
   },
   "outputs": [],
   "source": [
    "round_fun = lambda res: list(map(lambda k_v: (k_v[0],k_v[1], np.round(k_v[2], 4)), res))\n",
    "round_fun(dfLast.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlDjEgcz3SmP"
   },
   "source": [
    "### Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPqVZ3w63SmP"
   },
   "source": [
    "Estudia los 10 usuarios tienen un mayor número de seguidores en sus tweets.\n",
    "\n",
    "Se observa que el usuario con más seguidores fue Barack Obama (condidato demócrata) y que el candidatos republicano ni siquiera aparece en el top10. Esto demuestra que Obama hizo un mayor uso de las redes sociales (en concreto tweeter) para llegar a sus votantes y esta pudo ser la clave de su éxito. En el resto del top abundan los canales de noticias y alguna otra personalidad individual.\n",
    "\n",
    "Como observación decir que la columna de followers_count es muy variable, lo cual es extraño. Por ejemplo Barack Obama puede pasar de 600 a 2000 seguidores en menos de 10 minutos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V453q-Hq3SmP"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoKFrTPR3SmP"
   },
   "outputs": [],
   "source": [
    "dfusersFollowers = ###RELLENAR\n",
    "dfusersFollowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSoFlYio3SmP"
   },
   "source": [
    "### Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVFERnVl3SmP"
   },
   "source": [
    "Estudia la correlación entre el número de seguidores y los retweets. (Es extraño que la correlación sea negativa porque implica que a mayor número de followers menor numero de tweets. Es posible que esto sea debido a que la columna followers_count sea muy variable y tenga alguna incoherencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSp9x9Kv3SmQ"
   },
   "outputs": [],
   "source": [
    "correlation = ###RELLENAR\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdxBFs2g3SmQ"
   },
   "source": [
    "### Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi7NXPZ93SmQ"
   },
   "source": [
    "Estudiar cuales son los países con mayor número de followers. Para ello agrupar por países, habiendo filtrado los países que son Nulos. Mostrar por pantalla los 10 primeros y los 10 ultimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_300yjOa3SmQ"
   },
   "outputs": [],
   "source": [
    "paisesDesc = ###RELLENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhtQXnXU3SmQ"
   },
   "outputs": [],
   "source": [
    "paisesAsc = ###RELLENAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzIjHoXA3SmW"
   },
   "source": [
    "Los paises con mayor población cuentan con mayor número de tweets sin embargo India es una excepción. Esto se debe también al idioma utilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvmZcZP_3SmW"
   },
   "source": [
    "### Ejercicio 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6PABbtz3SmW"
   },
   "source": [
    "El siguiente estudio propone determinar la relacion entre los usuarios con más folowers (influencers) y las menciones de bandos políticos como pueden ser los demócratas de Obama y los republicanos de Rommey. Para ello se contará el número de tweets mencionando a alguno de los dos partidos entre Top 10 influencers de USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG4qEdDI3SmW"
   },
   "outputs": [],
   "source": [
    "#Dataframe auxiliar que cuenta con la información de los usuarios requerida\n",
    "aux = ###RELLENAR\n",
    "\n",
    "#Añadidos columna democrat.\n",
    "columnaDemocratDF = ###RELLENAR\n",
    "\n",
    "#Añadidos columna republican.\n",
    "columnaRepublicanDF = ###RELLENAR\n",
    "\n",
    "#Unimos los df, agrupamos las columnas democrat - repúblican y ordenamos la agrupación respecto al número de seguidores de los usuarios descendentemente\n",
    "dfLast = ###RELLENAR\n",
    "\n",
    "#Nos quedamos con el top10 tweets (cada fila es un tweet)\n",
    "dfLast.###RELLENAR\n",
    "dfLast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPWsRkcq3SmZ"
   },
   "source": [
    "Se observa que los usuarios con mayor número de seguidores, no twittean sobre política, y si lo hacen suele ser mencionando a ambos partidos. Los dos últimos tweets hacen referencia a los demócratas, sin embargo no sabemos si es para bien o para mal (para ello deberíamos pasar el filtro de insultos previo).\n",
    "\n",
    "Este estudio no es extrapolable puesto que contamos con una población pequeña de usuarios y tan solo estamos teniendo en cuenta 10 tweets.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
